nohup: ignoring input
wandb: Starting wandb agent üïµÔ∏è
2025-12-20 14:09:48,007 - wandb.wandb_agent - INFO - Running runs: []
2025-12-20 14:09:48,379 - wandb.wandb_agent - INFO - Agent received command: run
2025-12-20 14:09:48,379 - wandb.wandb_agent - INFO - Agent starting run with config:
	beta: 2
	dataset: lastfm
	device_id: 0
	lr: 0.001
2025-12-20 14:09:48,379 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python run_GBSR.py --beta=2 --dataset=lastfm --device_id=0 --lr=0.001
wandb: Currently logged in as: chtw2001 (chtw2001-hanyang-university). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/taekwon/SOTAs/GBSR/torch_version/wandb/run-20251220_140950-g6brqta2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lastfm_0.001_2.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: üßπ View sweep at https://wandb.ai/chtw2001-hanyang-university/GBSR/sweeps/t1zvf15e
wandb: üöÄ View run at https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/g6brqta2
saving log to  ../saved/lastfm/GBSR/0/
dataset=lastfm
runid=0
device_id=0
epochs=5000
batch_size=2048
lr=0.001
topk=20
early_stops=30
num_neg=1
gcn_layer=3
num_user=13024
num_item=22347
latent_dim=64
init_type=norm
l2_reg=0.0001
beta=2.0
sigma=0.25
edge_bias=0.5
social_noise_ratio=0
data_path=../datasets/lastfm/
Loading data from: ../datasets/lastfm/
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Successfully loaded social networks
Final Dataset Info: Users=13024, Items=22347, Total Nodes=35371
Total Training Interactions: 61978
Building negative sampler...
Negative sampler built.
Updated args.num_user to 13024
Updated args.num_item to 22347
user_embeddings.weight torch.Size([13024, 64])
item_embeddings.weight torch.Size([22347, 64])
linear_1.weight torch.Size([64, 128])
linear_1.bias torch.Size([64])
linear_2.weight torch.Size([1, 64])
linear_2.bias torch.Size([1])
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  1.87it/s][A
All_batch: 3it [00:00,  5.07it/s][A
All_batch: 5it [00:00,  8.21it/s][A2025-12-20 14:09:53,390 - wandb.wandb_agent - INFO - Running runs: ['g6brqta2']

All_batch: 7it [00:00,  9.96it/s][A
All_batch: 9it [00:01, 11.84it/s][A
All_batch: 11it [00:01, 13.15it/s][A
All_batch: 13it [00:01, 13.73it/s][A
All_batch: 15it [00:01, 14.63it/s][A
All_batch: 17it [00:01, 14.82it/s][A
All_batch: 19it [00:01, 15.39it/s][A
All_batch: 21it [00:01, 15.34it/s][A
All_batch: 23it [00:01, 15.62it/s][A
All_batch: 25it [00:02, 15.36it/s][A
All_batch: 27it [00:02, 15.72it/s][A
All_batch: 29it [00:02, 15.48it/s][A
All_batch: 31it [00:02, 16.54it/s][AAll_batch: 31it [00:02, 12.81it/s]
[1;34mEpoch:0, Train_AUC:0.7395, Loss_rank:0.6931, Loss_reg:0.0000, Loss_ib:0.0037, Loss_sum:0.6968
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:0, topk:20, recall:0.0210, ndcg:0.0144, prec:0.0041, mrr:0.0232
[0m[1;31mBest Eval: Epoch:0, topk:20, recall:0.0210, ndcg:0.0144
[0mSaved model to ../saved/lastfm/GBSR/0/models/epoch_0_ndcg_0.01437226.ckpt
traintime:2.4198, valtime:1.2324

[1;35mTrain:[0m:   0%|[33m          [0m| 1/5000 [00:03<5:04:17,  3.65s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 2it [00:00, 19.28it/s][A
All_batch: 4it [00:00, 15.24it/s][A
All_batch: 7it [00:00, 16.47it/s][A
All_batch: 9it [00:00, 16.04it/s][A
All_batch: 11it [00:00, 16.26it/s][A
All_batch: 13it [00:00, 15.71it/s][A
All_batch: 15it [00:00, 16.14it/s][A
All_batch: 17it [00:01, 15.74it/s][A
All_batch: 19it [00:01, 16.80it/s][A
All_batch: 21it [00:01, 15.28it/s][A
All_batch: 24it [00:01, 16.08it/s][A
All_batch: 26it [00:01, 15.76it/s][A
All_batch: 28it [00:01, 15.96it/s][A
All_batch: 30it [00:01, 15.65it/s][AAll_batch: 31it [00:01, 16.05it/s]
[1;34mEpoch:1, Train_AUC:0.8894, Loss_rank:0.6930, Loss_reg:0.0000, Loss_ib:0.0037, Loss_sum:0.6966
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:1, topk:20, recall:0.0809, ndcg:0.0573, prec:0.0164, mrr:0.0928
[0m[1;31mBest Eval: Epoch:1, topk:20, recall:0.0809, ndcg:0.0573
[0mSaved model to ../saved/lastfm/GBSR/0/models/epoch_1_ndcg_0.05725224.ckpt
traintime:1.9324, valtime:1.2474

[1;35mTrain:[0m:   0%|[33m          [0m| 2/5000 [00:06<4:41:05,  3.37s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 2it [00:00, 18.74it/s][A
All_batch: 4it [00:00, 16.48it/s][A
All_batch: 6it [00:00, 14.98it/s][A
All_batch: 8it [00:00, 14.94it/s][A
All_batch: 10it [00:00, 13.88it/s][A
All_batch: 12it [00:00, 14.39it/s][A
All_batch: 14it [00:00, 14.20it/s][A
All_batch: 16it [00:01, 14.29it/s][A
All_batch: 18it [00:01, 14.16it/s][A
All_batch: 20it [00:01, 13.90it/s][A
All_batch: 22it [00:01, 14.47it/s][A
All_batch: 24it [00:01, 14.39it/s][A
All_batch: 26it [00:01, 14.24it/s][A
All_batch: 28it [00:01, 14.26it/s][A
All_batch: 30it [00:02, 14.05it/s][AAll_batch: 31it [00:02, 14.46it/s]
[1;34mEpoch:2, Train_AUC:0.9763, Loss_rank:0.6923, Loss_reg:0.0000, Loss_ib:0.0038, Loss_sum:0.6961
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712, prec:0.0509, mrr:0.2492
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mSaved model to ../saved/lastfm/GBSR/0/models/epoch_2_ndcg_0.17119137.ckpt
traintime:2.1445, valtime:1.1956

[1;35mTrain:[0m:   0%|[33m          [0m| 3/5000 [00:10<4:39:44,  3.36s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  9.66it/s][A
All_batch: 2it [00:00,  7.50it/s][A
All_batch: 3it [00:00,  7.15it/s][A
All_batch: 4it [00:00,  7.04it/s][A
All_batch: 5it [00:00,  6.69it/s][A
All_batch: 6it [00:00,  6.88it/s][A
All_batch: 7it [00:01,  6.67it/s][A
All_batch: 8it [00:01,  6.79it/s][A
All_batch: 9it [00:01,  6.58it/s][A
All_batch: 10it [00:01,  6.90it/s][A
All_batch: 11it [00:01,  6.51it/s][A
All_batch: 12it [00:01,  6.88it/s][A
All_batch: 13it [00:01,  6.96it/s][A
All_batch: 14it [00:02,  6.68it/s][A
All_batch: 15it [00:02,  6.85it/s][A
All_batch: 16it [00:02,  6.66it/s][A
All_batch: 17it [00:02,  6.76it/s][A
All_batch: 18it [00:02,  6.58it/s][A
All_batch: 19it [00:02,  6.72it/s][A
All_batch: 20it [00:02,  6.54it/s][A
All_batch: 21it [00:03,  6.92it/s][A
All_batch: 22it [00:03,  6.97it/s][A
All_batch: 23it [00:03,  6.69it/s][A
All_batch: 24it [00:03,  6.87it/s][A
All_batch: 25it [00:03,  6.67it/s][A
All_batch: 26it [00:03,  6.82it/s][A
All_batch: 27it [00:03,  6.57it/s][A
All_batch: 28it [00:04,  6.94it/s][A
All_batch: 29it [00:04,  7.04it/s][A
All_batch: 30it [00:04,  6.75it/s][A
All_batch: 31it [00:04,  6.89it/s][AAll_batch: 31it [00:04,  6.82it/s]
[1;34mEpoch:3, Train_AUC:0.9938, Loss_rank:0.6842, Loss_reg:0.0000, Loss_ib:0.0061, Loss_sum:0.6903
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:3, topk:20, recall:0.1303, ndcg:0.0917, prec:0.0286, mrr:0.1428
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:4.5453, valtime:1.1961

[1;35mTrain:[0m:   0%|[33m          [0m| 4/5000 [00:15<5:58:00,  4.30s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  6.01it/s][A
All_batch: 2it [00:00,  6.81it/s][A
All_batch: 3it [00:00,  6.51it/s][A
All_batch: 4it [00:00,  6.77it/s][A
All_batch: 5it [00:00,  6.57it/s][A
All_batch: 6it [00:00,  6.98it/s][A
All_batch: 7it [00:01,  6.97it/s][A
All_batch: 8it [00:01,  6.76it/s][A
All_batch: 9it [00:01,  6.79it/s][A
All_batch: 10it [00:01,  6.52it/s][A
All_batch: 11it [00:01,  6.63it/s][A
All_batch: 12it [00:01,  6.38it/s][A
All_batch: 13it [00:01,  6.49it/s][A
All_batch: 14it [00:02,  6.34it/s][A
All_batch: 15it [00:02,  6.46it/s][A
All_batch: 16it [00:02,  6.25it/s][A
All_batch: 17it [00:02,  6.57it/s][A
All_batch: 18it [00:02,  6.64it/s][A
All_batch: 19it [00:02,  6.43it/s][A
All_batch: 20it [00:03,  6.51it/s][A
All_batch: 21it [00:03,  6.29it/s][A
All_batch: 22it [00:03,  6.47it/s][A
All_batch: 23it [00:03,  6.34it/s][A
All_batch: 24it [00:03,  6.43it/s][A
All_batch: 25it [00:03,  6.26it/s][A
All_batch: 26it [00:03,  6.60it/s][A
All_batch: 27it [00:04,  6.66it/s][A
All_batch: 28it [00:04,  5.20it/s][A
All_batch: 29it [00:04,  4.60it/s][A
All_batch: 30it [00:04,  4.21it/s][A
All_batch: 31it [00:05,  3.90it/s][AAll_batch: 31it [00:05,  5.87it/s]
[1;34mEpoch:4, Train_AUC:0.9742, Loss_rank:0.5554, Loss_reg:0.0000, Loss_ib:0.0077, Loss_sum:0.5632
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:4, topk:20, recall:0.1182, ndcg:0.0777, prec:0.0260, mrr:0.1137
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:5.2796, valtime:1.1925

[1;35mTrain:[0m:   0%|[33m          [0m| 5/5000 [00:22<7:03:10,  5.08s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.56it/s][A
All_batch: 2it [00:00,  3.24it/s][A
All_batch: 3it [00:00,  3.44it/s][A
All_batch: 4it [00:01,  3.55it/s][A
All_batch: 5it [00:01,  3.32it/s][A
All_batch: 6it [00:01,  3.45it/s][A
All_batch: 7it [00:02,  3.60it/s][A
All_batch: 8it [00:02,  3.36it/s][A
All_batch: 9it [00:02,  3.45it/s][A
All_batch: 10it [00:02,  3.62it/s][A
All_batch: 11it [00:03,  3.40it/s][A
All_batch: 12it [00:03,  3.46it/s][A
All_batch: 13it [00:03,  3.63it/s][A
All_batch: 14it [00:04,  3.38it/s][A
All_batch: 15it [00:04,  3.41it/s][A
All_batch: 16it [00:04,  3.60it/s][A
All_batch: 17it [00:04,  3.41it/s][A
All_batch: 18it [00:05,  3.39it/s][A
All_batch: 19it [00:05,  3.56it/s][A
All_batch: 20it [00:05,  3.52it/s][A
All_batch: 21it [00:06,  3.47it/s][A
All_batch: 22it [00:06,  3.53it/s][A
All_batch: 23it [00:06,  3.50it/s][A
All_batch: 24it [00:06,  3.41it/s][A
All_batch: 25it [00:07,  3.52it/s][A
All_batch: 26it [00:07,  3.59it/s][A
All_batch: 27it [00:07,  3.38it/s][A
All_batch: 28it [00:08,  3.46it/s][A
All_batch: 29it [00:08,  3.63it/s][A
All_batch: 30it [00:08,  3.39it/s][A
All_batch: 31it [00:08,  3.43it/s][AAll_batch: 31it [00:08,  3.47it/s]
[1;34mEpoch:5, Train_AUC:0.9679, Loss_rank:0.2969, Loss_reg:0.0001, Loss_ib:0.0013, Loss_sum:0.2983
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:5, topk:20, recall:0.1181, ndcg:0.0773, prec:0.0260, mrr:0.1131
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.9346, valtime:1.2089

[1;35mTrain:[0m:   0%|[33m          [0m| 6/5000 [00:32<9:26:17,  6.80s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.43it/s][A
All_batch: 2it [00:00,  3.31it/s][A
All_batch: 3it [00:00,  3.47it/s][A
All_batch: 4it [00:01,  3.51it/s][A
All_batch: 5it [00:01,  3.45it/s][A
All_batch: 6it [00:01,  3.52it/s][A
All_batch: 7it [00:02,  3.52it/s][A
All_batch: 8it [00:02,  3.48it/s][A
All_batch: 9it [00:02,  3.48it/s][A
All_batch: 10it [00:02,  3.51it/s][A
All_batch: 11it [00:03,  3.45it/s][A
All_batch: 12it [00:03,  3.46it/s][A
All_batch: 13it [00:03,  3.53it/s][A
All_batch: 14it [00:04,  3.44it/s][A
All_batch: 15it [00:04,  3.63it/s][A
All_batch: 16it [00:04,  3.57it/s][A
All_batch: 17it [00:04,  3.40it/s][A
All_batch: 18it [00:05,  3.58it/s][A
All_batch: 19it [00:05,  3.56it/s][A
All_batch: 20it [00:05,  3.48it/s][A
All_batch: 21it [00:05,  3.57it/s][A
All_batch: 22it [00:06,  3.49it/s][A
All_batch: 23it [00:06,  3.42it/s][A
All_batch: 24it [00:06,  3.45it/s][A
All_batch: 25it [00:07,  3.50it/s][A
All_batch: 26it [00:07,  3.43it/s][A
All_batch: 27it [00:07,  3.42it/s][A
All_batch: 28it [00:08,  3.49it/s][A
All_batch: 29it [00:08,  3.45it/s][A
All_batch: 30it [00:08,  3.43it/s][A
All_batch: 31it [00:08,  3.51it/s][AAll_batch: 31it [00:08,  3.49it/s]
[1;34mEpoch:6, Train_AUC:0.9650, Loss_rank:0.1773, Loss_reg:0.0002, Loss_ib:0.0005, Loss_sum:0.1780
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:6, topk:20, recall:0.1168, ndcg:0.0763, prec:0.0257, mrr:0.1114
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.8918, valtime:1.2018

[1;35mTrain:[0m:   0%|[33m          [0m| 7/5000 [00:42<10:55:41,  7.88s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.55it/s][A
All_batch: 2it [00:00,  3.81it/s][A
All_batch: 3it [00:00,  3.34it/s][A
All_batch: 4it [00:01,  3.45it/s][A
All_batch: 5it [00:01,  3.65it/s][A
All_batch: 6it [00:01,  3.36it/s][A
All_batch: 7it [00:02,  3.36it/s][A
All_batch: 8it [00:02,  3.53it/s][A
All_batch: 9it [00:02,  3.38it/s][A
All_batch: 10it [00:02,  3.38it/s][A
All_batch: 11it [00:03,  3.59it/s][A
All_batch: 12it [00:03,  3.48it/s][A
All_batch: 13it [00:03,  3.38it/s][A
All_batch: 14it [00:04,  3.56it/s][A
All_batch: 15it [00:04,  3.53it/s][A
All_batch: 16it [00:04,  3.49it/s][A
All_batch: 17it [00:04,  3.53it/s][A
All_batch: 18it [00:05,  3.64it/s][A
All_batch: 19it [00:05,  3.51it/s][A
All_batch: 20it [00:05,  3.54it/s][A
All_batch: 21it [00:05,  3.54it/s][A
All_batch: 22it [00:06,  3.35it/s][A
All_batch: 23it [00:06,  3.38it/s][A
All_batch: 24it [00:06,  3.50it/s][A
All_batch: 25it [00:07,  3.38it/s][A
All_batch: 26it [00:07,  3.40it/s][A
All_batch: 27it [00:07,  3.49it/s][A
All_batch: 28it [00:08,  3.35it/s][A
All_batch: 29it [00:08,  3.36it/s][A
All_batch: 30it [00:08,  3.48it/s][A
All_batch: 31it [00:08,  3.36it/s][AAll_batch: 31it [00:08,  3.46it/s]
[1;34mEpoch:7, Train_AUC:0.9666, Loss_rank:0.1359, Loss_reg:0.0003, Loss_ib:0.0003, Loss_sum:0.1365
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:7, topk:20, recall:0.1177, ndcg:0.0768, prec:0.0260, mrr:0.1120
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.9713, valtime:1.1907

[1;35mTrain:[0m:   0%|[33m          [0m| 8/5000 [00:52<11:56:01,  8.61s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.31it/s][A
All_batch: 2it [00:00,  3.18it/s][A
All_batch: 3it [00:00,  3.41it/s][A
All_batch: 4it [00:01,  3.41it/s][A
All_batch: 5it [00:01,  3.25it/s][A
All_batch: 6it [00:01,  3.33it/s][A
All_batch: 7it [00:02,  3.41it/s][A
All_batch: 8it [00:02,  3.28it/s][A
All_batch: 9it [00:02,  3.60it/s][A
All_batch: 10it [00:02,  3.91it/s][A
All_batch: 11it [00:03,  3.85it/s][A
All_batch: 12it [00:03,  4.02it/s][A
All_batch: 13it [00:03,  4.28it/s][A
All_batch: 14it [00:03,  4.13it/s][A
All_batch: 15it [00:04,  4.19it/s][A
All_batch: 16it [00:04,  4.41it/s][A
All_batch: 17it [00:04,  4.23it/s][A
All_batch: 18it [00:04,  4.23it/s][A
All_batch: 19it [00:04,  4.40it/s][A
All_batch: 20it [00:05,  4.32it/s][A
All_batch: 21it [00:05,  4.29it/s][A
All_batch: 22it [00:05,  4.43it/s][A
All_batch: 23it [00:05,  4.37it/s][A
All_batch: 24it [00:06,  4.24it/s][A
All_batch: 25it [00:06,  4.40it/s][A
All_batch: 26it [00:06,  4.41it/s][A
All_batch: 27it [00:06,  4.25it/s][A
All_batch: 28it [00:06,  4.42it/s][A
All_batch: 29it [00:07,  4.41it/s][A
All_batch: 30it [00:07,  4.23it/s][A
All_batch: 31it [00:07,  4.45it/s][AAll_batch: 31it [00:07,  4.04it/s]
[1;34mEpoch:8, Train_AUC:0.9661, Loss_rank:0.1192, Loss_reg:0.0003, Loss_ib:0.0002, Loss_sum:0.1197
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:8, topk:20, recall:0.1181, ndcg:0.0767, prec:0.0260, mrr:0.1114
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.6771, valtime:1.2171

[1;35mTrain:[0m:   0%|[33m          [0m| 9/5000 [01:01<12:03:27,  8.70s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.38it/s][A
All_batch: 2it [00:00,  3.38it/s][A
All_batch: 3it [00:00,  3.54it/s][A
All_batch: 4it [00:01,  3.37it/s][A
All_batch: 5it [00:01,  3.39it/s][A
All_batch: 6it [00:01,  3.53it/s][A
All_batch: 7it [00:02,  3.42it/s][A
All_batch: 8it [00:02,  3.40it/s][A
All_batch: 9it [00:02,  3.49it/s][A
All_batch: 10it [00:02,  3.41it/s][A
All_batch: 11it [00:03,  3.35it/s][A
All_batch: 12it [00:03,  3.43it/s][A
All_batch: 13it [00:03,  3.42it/s][A
All_batch: 14it [00:04,  3.35it/s][A
All_batch: 15it [00:04,  3.44it/s][A
All_batch: 16it [00:04,  3.46it/s][A
All_batch: 17it [00:04,  3.31it/s][A
All_batch: 18it [00:05,  3.45it/s][A
All_batch: 19it [00:05,  3.45it/s][A
All_batch: 20it [00:05,  3.32it/s][A
All_batch: 21it [00:06,  3.42it/s][A
All_batch: 22it [00:06,  3.46it/s][A
All_batch: 23it [00:06,  3.31it/s][A
All_batch: 24it [00:07,  3.37it/s][A
All_batch: 25it [00:07,  3.51it/s][A
All_batch: 26it [00:07,  3.35it/s][A
All_batch: 27it [00:07,  3.36it/s][A
All_batch: 28it [00:08,  3.50it/s][A
All_batch: 29it [00:08,  3.37it/s][A
All_batch: 30it [00:08,  3.38it/s][A
All_batch: 31it [00:09,  3.48it/s][AAll_batch: 31it [00:09,  3.42it/s]
[1;34mEpoch:9, Train_AUC:0.9654, Loss_rank:0.1103, Loss_reg:0.0003, Loss_ib:0.0002, Loss_sum:0.1109
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:9, topk:20, recall:0.1184, ndcg:0.0764, prec:0.0261, mrr:0.1102
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:9.0731, valtime:1.2719

[1;35mTrain:[0m:   0%|[33m          [0m| 10/5000 [01:12<12:45:37,  9.21s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.69it/s][A
All_batch: 2it [00:00,  3.60it/s][A
All_batch: 3it [00:00,  3.32it/s][A
All_batch: 4it [00:01,  3.44it/s][A
All_batch: 5it [00:01,  3.54it/s][A
All_batch: 6it [00:01,  3.34it/s][A
All_batch: 7it [00:02,  3.45it/s][A
All_batch: 8it [00:02,  3.55it/s][A
All_batch: 9it [00:02,  3.36it/s][A
All_batch: 10it [00:02,  3.38it/s][A
All_batch: 11it [00:03,  3.49it/s][A
All_batch: 12it [00:03,  3.38it/s][A
All_batch: 13it [00:03,  3.37it/s][A
All_batch: 14it [00:04,  3.47it/s][A
All_batch: 15it [00:04,  3.43it/s][A
All_batch: 16it [00:04,  3.39it/s][A
All_batch: 17it [00:04,  3.49it/s][A
All_batch: 18it [00:05,  3.46it/s][A
All_batch: 19it [00:05,  3.40it/s][A
All_batch: 20it [00:05,  3.50it/s][A
All_batch: 21it [00:06,  3.48it/s][A
All_batch: 22it [00:06,  3.35it/s][A
All_batch: 23it [00:06,  3.46it/s][A
All_batch: 24it [00:06,  3.44it/s][A
All_batch: 25it [00:07,  3.32it/s][A
All_batch: 26it [00:07,  3.45it/s][A
All_batch: 27it [00:07,  3.45it/s][A
All_batch: 28it [00:08,  3.32it/s][A
All_batch: 29it [00:08,  3.39it/s][A
All_batch: 30it [00:08,  3.49it/s][A
All_batch: 31it [00:09,  3.33it/s][AAll_batch: 31it [00:09,  3.42it/s]
[1;34mEpoch:10, Train_AUC:0.9654, Loss_rank:0.1058, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.1063
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:10, topk:20, recall:0.1178, ndcg:0.0764, prec:0.0261, mrr:0.1105
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:9.0632, valtime:1.2022

[1;35mTrain:[0m:   0%|[33m          [0m| 11/5000 [01:22<13:12:26,  9.53s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.41it/s][A
All_batch: 2it [00:00,  3.21it/s][A
All_batch: 3it [00:00,  3.45it/s][A
All_batch: 4it [00:01,  3.45it/s][A
All_batch: 5it [00:01,  3.29it/s][A
All_batch: 6it [00:01,  3.40it/s][A
All_batch: 7it [00:02,  3.48it/s][A
All_batch: 8it [00:02,  3.33it/s][A
All_batch: 9it [00:02,  3.40it/s][A
All_batch: 10it [00:02,  3.49it/s][A
All_batch: 11it [00:03,  3.34it/s][A
All_batch: 12it [00:03,  3.36it/s][A
All_batch: 13it [00:03,  3.47it/s][A
All_batch: 14it [00:04,  3.35it/s][A
All_batch: 15it [00:04,  3.34it/s][A
All_batch: 16it [00:04,  3.44it/s][A
All_batch: 17it [00:05,  3.42it/s][A
All_batch: 18it [00:05,  3.39it/s][A
All_batch: 19it [00:05,  3.50it/s][A
All_batch: 20it [00:05,  3.44it/s][A
All_batch: 21it [00:06,  3.36it/s][A
All_batch: 22it [00:06,  3.49it/s][A
All_batch: 23it [00:06,  3.47it/s][A
All_batch: 24it [00:07,  3.34it/s][A
All_batch: 25it [00:07,  3.46it/s][A
All_batch: 26it [00:07,  3.46it/s][A
All_batch: 27it [00:07,  3.35it/s][A
All_batch: 28it [00:08,  3.49it/s][A
All_batch: 29it [00:08,  3.49it/s][A
All_batch: 30it [00:08,  3.37it/s][A
All_batch: 31it [00:09,  3.72it/s][AAll_batch: 31it [00:09,  3.44it/s]
[1;34mEpoch:11, Train_AUC:0.9662, Loss_rank:0.1000, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.1006
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:11, topk:20, recall:0.1173, ndcg:0.0760, prec:0.0260, mrr:0.1099
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:9.0125, valtime:1.2405

[1;35mTrain:[0m:   0%|[33m          [0m| 12/5000 [01:32<13:30:33,  9.75s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  7.46it/s][A
All_batch: 2it [00:00,  6.65it/s][A
All_batch: 3it [00:00,  6.61it/s][A
All_batch: 4it [00:00,  6.45it/s][A
All_batch: 5it [00:00,  6.31it/s][A
All_batch: 6it [00:00,  6.22it/s][A
All_batch: 7it [00:01,  6.33it/s][A
All_batch: 8it [00:01,  6.30it/s][A
All_batch: 9it [00:01,  6.21it/s][A
All_batch: 10it [00:01,  6.17it/s][A
All_batch: 11it [00:01,  6.09it/s][A
All_batch: 12it [00:01,  6.04it/s][A
All_batch: 13it [00:02,  6.02it/s][A
All_batch: 14it [00:02,  6.01it/s][A
All_batch: 15it [00:02,  6.00it/s][A
All_batch: 16it [00:02,  6.01it/s][A
All_batch: 17it [00:02,  5.98it/s][A
All_batch: 18it [00:02,  6.03it/s][A
All_batch: 19it [00:03,  6.04it/s][A
All_batch: 20it [00:03,  6.05it/s][A
All_batch: 21it [00:03,  6.06it/s][A
All_batch: 22it [00:03,  6.06it/s][A
All_batch: 23it [00:03,  6.03it/s][A
All_batch: 24it [00:03,  6.02it/s][A
All_batch: 25it [00:04,  6.01it/s][A
All_batch: 26it [00:04,  6.02it/s][A
All_batch: 27it [00:04,  5.51it/s][A
All_batch: 28it [00:04,  4.61it/s][A
All_batch: 29it [00:05,  4.01it/s][A
All_batch: 30it [00:05,  3.94it/s][A
All_batch: 31it [00:05,  3.79it/s][AAll_batch: 31it [00:05,  5.50it/s]
[1;34mEpoch:12, Train_AUC:0.9647, Loss_rank:0.1014, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.1019
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:12, topk:20, recall:0.1171, ndcg:0.0759, prec:0.0259, mrr:0.1100
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:5.6386, valtime:1.2004

[1;35mTrain:[0m:   0%|[33m          [0m| 13/5000 [01:39<12:17:06,  8.87s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.51it/s][A
All_batch: 2it [00:00,  3.21it/s][A
All_batch: 3it [00:00,  3.31it/s][A
All_batch: 4it [00:01,  3.51it/s][A
All_batch: 5it [00:01,  3.34it/s][A
All_batch: 6it [00:01,  3.36it/s][A
All_batch: 7it [00:02,  3.51it/s][A
All_batch: 8it [00:02,  3.41it/s][A
All_batch: 9it [00:02,  3.41it/s][A
All_batch: 10it [00:02,  3.50it/s][A
All_batch: 11it [00:03,  3.41it/s][A
All_batch: 12it [00:03,  3.37it/s][A
All_batch: 13it [00:03,  3.48it/s][A
All_batch: 14it [00:04,  3.45it/s][A
All_batch: 15it [00:04,  3.41it/s][A
All_batch: 16it [00:04,  3.49it/s][A
All_batch: 17it [00:04,  3.47it/s][A
All_batch: 18it [00:05,  3.40it/s][A
All_batch: 19it [00:05,  3.52it/s][A
All_batch: 20it [00:05,  3.52it/s][A
All_batch: 21it [00:06,  3.37it/s][A
All_batch: 22it [00:06,  3.49it/s][A
All_batch: 23it [00:06,  3.45it/s][A
All_batch: 24it [00:07,  3.32it/s][A
All_batch: 25it [00:07,  3.39it/s][A
All_batch: 26it [00:07,  3.48it/s][A
All_batch: 27it [00:07,  3.32it/s][A
All_batch: 28it [00:08,  3.37it/s][A
All_batch: 29it [00:08,  3.50it/s][A
All_batch: 30it [00:08,  3.36it/s][A
All_batch: 31it [00:09,  3.37it/s][AAll_batch: 31it [00:09,  3.42it/s]
[1;34mEpoch:13, Train_AUC:0.9655, Loss_rank:0.0990, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0996
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:13, topk:20, recall:0.1167, ndcg:0.0758, prec:0.0258, mrr:0.1101
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:9.0715, valtime:1.1860

[1;35mTrain:[0m:   0%|[33m          [0m| 14/5000 [01:49<12:51:49,  9.29s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.27it/s][A
All_batch: 2it [00:00,  3.55it/s][A
All_batch: 3it [00:00,  3.43it/s][A
All_batch: 4it [00:01,  3.32it/s][A
All_batch: 5it [00:01,  3.47it/s][A
All_batch: 6it [00:01,  3.44it/s][A
All_batch: 7it [00:02,  3.33it/s][A
All_batch: 8it [00:02,  3.44it/s][A
All_batch: 9it [00:02,  3.44it/s][A
All_batch: 10it [00:02,  3.31it/s][A
All_batch: 11it [00:03,  3.47it/s][A
All_batch: 12it [00:03,  3.47it/s][A
All_batch: 13it [00:03,  3.34it/s][A
All_batch: 14it [00:04,  3.48it/s][A
All_batch: 15it [00:04,  3.47it/s][A
All_batch: 16it [00:04,  3.34it/s][A
All_batch: 17it [00:04,  3.44it/s][A
All_batch: 18it [00:05,  3.52it/s][A
All_batch: 19it [00:05,  3.35it/s][A
All_batch: 20it [00:05,  3.37it/s][A
All_batch: 21it [00:06,  3.48it/s][A
All_batch: 22it [00:06,  3.36it/s][A
All_batch: 23it [00:06,  3.38it/s][A
All_batch: 24it [00:07,  3.47it/s][A
All_batch: 25it [00:07,  3.41it/s][A
All_batch: 26it [00:07,  3.40it/s][A
All_batch: 27it [00:07,  3.50it/s][A
All_batch: 28it [00:08,  3.48it/s][A
All_batch: 29it [00:08,  3.69it/s][A
All_batch: 30it [00:08,  4.03it/s][A
All_batch: 31it [00:08,  4.06it/s][AAll_batch: 31it [00:08,  3.50it/s]
[1;34mEpoch:14, Train_AUC:0.9670, Loss_rank:0.0948, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0954
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:14, topk:20, recall:0.1166, ndcg:0.0759, prec:0.0258, mrr:0.1103
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.8469, valtime:1.1468

[1;35mTrain:[0m:   0%|[33m          [0m| 15/5000 [01:59<13:09:21,  9.50s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  5.07it/s][A
All_batch: 2it [00:00,  5.79it/s][A
All_batch: 3it [00:00,  4.95it/s][A
All_batch: 4it [00:00,  5.03it/s][A
All_batch: 5it [00:00,  5.01it/s][A
All_batch: 6it [00:01,  4.56it/s][A
All_batch: 7it [00:01,  4.74it/s][A
All_batch: 8it [00:01,  4.81it/s][A
All_batch: 9it [00:01,  4.43it/s][A
All_batch: 10it [00:02,  4.61it/s][A
All_batch: 11it [00:02,  4.73it/s][A
All_batch: 12it [00:02,  4.46it/s][A
All_batch: 13it [00:02,  4.59it/s][A
All_batch: 14it [00:02,  4.71it/s][A
All_batch: 15it [00:03,  4.37it/s][A
All_batch: 16it [00:03,  4.58it/s][A
All_batch: 17it [00:03,  4.70it/s][A
All_batch: 18it [00:03,  4.43it/s][A
All_batch: 19it [00:04,  4.69it/s][A
All_batch: 20it [00:04,  4.80it/s][A
All_batch: 21it [00:04,  4.64it/s][A
All_batch: 22it [00:04,  4.64it/s][A
All_batch: 23it [00:05,  4.14it/s][A
All_batch: 24it [00:05,  3.83it/s][A
All_batch: 25it [00:05,  3.77it/s][A
All_batch: 26it [00:05,  3.74it/s][A
All_batch: 27it [00:06,  3.56it/s][A
All_batch: 28it [00:06,  3.62it/s][A
All_batch: 29it [00:06,  3.69it/s][A
All_batch: 30it [00:07,  3.50it/s][A
All_batch: 31it [00:07,  3.51it/s][AAll_batch: 31it [00:07,  4.25it/s]
[1;34mEpoch:15, Train_AUC:0.9665, Loss_rank:0.0950, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0956
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:15, topk:20, recall:0.1171, ndcg:0.0770, prec:0.0258, mrr:0.1136
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.2957, valtime:1.2005

[1;35mTrain:[0m:   0%|[33m          [0m| 16/5000 [02:08<12:44:05,  9.20s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.24it/s][A
All_batch: 2it [00:00,  3.50it/s][A
All_batch: 3it [00:00,  3.58it/s][A
All_batch: 4it [00:01,  3.40it/s][A
All_batch: 5it [00:01,  3.49it/s][A
All_batch: 6it [00:01,  3.63it/s][A
All_batch: 7it [00:02,  3.38it/s][A
All_batch: 8it [00:02,  3.43it/s][A
All_batch: 9it [00:02,  3.60it/s][A
All_batch: 10it [00:02,  3.39it/s][A
All_batch: 11it [00:03,  3.40it/s][A
All_batch: 12it [00:03,  3.57it/s][A
All_batch: 13it [00:03,  3.41it/s][A
All_batch: 14it [00:04,  3.42it/s][A
All_batch: 15it [00:04,  3.54it/s][A
All_batch: 16it [00:04,  3.44it/s][A
All_batch: 17it [00:04,  3.43it/s][A
All_batch: 18it [00:05,  3.62it/s][A
All_batch: 19it [00:05,  3.46it/s][A
All_batch: 20it [00:05,  3.43it/s][A
All_batch: 21it [00:06,  3.58it/s][A
All_batch: 22it [00:06,  3.54it/s][A
All_batch: 23it [00:06,  3.36it/s][A
All_batch: 24it [00:06,  3.55it/s][A
All_batch: 25it [00:07,  3.53it/s][A
All_batch: 26it [00:07,  3.46it/s][A
All_batch: 27it [00:07,  3.56it/s][A
All_batch: 28it [00:08,  3.49it/s][A
All_batch: 29it [00:08,  3.41it/s][A
All_batch: 30it [00:08,  3.46it/s][A
All_batch: 31it [00:08,  3.50it/s][AAll_batch: 31it [00:08,  3.48it/s]
[1;34mEpoch:16, Train_AUC:0.9652, Loss_rank:0.0975, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0981
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:16, topk:20, recall:0.1171, ndcg:0.0763, prec:0.0258, mrr:0.1114
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.9065, valtime:1.1808

[1;35mTrain:[0m:   0%|[33m          [0m| 17/5000 [02:18<13:06:08,  9.47s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.24it/s][A
All_batch: 2it [00:00,  3.55it/s][A
All_batch: 3it [00:00,  3.57it/s][A
All_batch: 4it [00:01,  3.67it/s][A
All_batch: 5it [00:01,  3.58it/s][A
All_batch: 6it [00:01,  3.48it/s][A
All_batch: 7it [00:01,  3.56it/s][A
All_batch: 8it [00:02,  3.57it/s][A
All_batch: 9it [00:02,  3.40it/s][A
All_batch: 10it [00:02,  3.52it/s][A
All_batch: 11it [00:03,  3.57it/s][A
All_batch: 12it [00:03,  3.46it/s][A
All_batch: 13it [00:03,  3.57it/s][A
All_batch: 14it [00:03,  3.57it/s][A
All_batch: 15it [00:04,  3.41it/s][A
All_batch: 16it [00:04,  3.50it/s][A
All_batch: 17it [00:04,  3.54it/s][A
All_batch: 18it [00:05,  3.45it/s][A
All_batch: 19it [00:05,  3.50it/s][A
All_batch: 20it [00:05,  3.53it/s][A
All_batch: 21it [00:05,  3.43it/s][A
All_batch: 22it [00:06,  3.53it/s][A
All_batch: 23it [00:06,  3.53it/s][A
All_batch: 24it [00:06,  3.47it/s][A
All_batch: 25it [00:07,  3.48it/s][A
All_batch: 26it [00:07,  3.51it/s][A
All_batch: 27it [00:07,  3.43it/s][A
All_batch: 28it [00:07,  3.45it/s][A
All_batch: 29it [00:08,  3.55it/s][A
All_batch: 30it [00:08,  3.41it/s][A
All_batch: 31it [00:08,  3.58it/s][AAll_batch: 31it [00:08,  3.52it/s]
[1;34mEpoch:17, Train_AUC:0.9652, Loss_rank:0.0971, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0977
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:17, topk:20, recall:0.1171, ndcg:0.0763, prec:0.0259, mrr:0.1112
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.8095, valtime:1.2292

[1;35mTrain:[0m:   0%|[33m          [0m| 18/5000 [02:28<13:20:16,  9.64s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  2.96it/s][A
All_batch: 2it [00:00,  3.41it/s][A
All_batch: 3it [00:00,  3.56it/s][A
All_batch: 4it [00:01,  3.33it/s][A
All_batch: 5it [00:01,  3.44it/s][A
All_batch: 6it [00:01,  3.54it/s][A
All_batch: 7it [00:02,  3.38it/s][A
All_batch: 8it [00:02,  3.38it/s][A
All_batch: 9it [00:02,  3.61it/s][A
All_batch: 10it [00:02,  3.43it/s][A
All_batch: 11it [00:03,  3.50it/s][A
All_batch: 12it [00:03,  3.59it/s][A
All_batch: 13it [00:03,  3.50it/s][A
All_batch: 14it [00:04,  3.38it/s][A
All_batch: 15it [00:04,  3.46it/s][A
All_batch: 16it [00:04,  3.48it/s][A
All_batch: 17it [00:04,  3.40it/s][A
All_batch: 18it [00:05,  3.48it/s][A
All_batch: 19it [00:05,  3.50it/s][A
All_batch: 20it [00:05,  3.38it/s][A
All_batch: 21it [00:06,  3.47it/s][A
All_batch: 22it [00:06,  3.51it/s][A
All_batch: 23it [00:06,  3.35it/s][A
All_batch: 24it [00:06,  3.46it/s][A
All_batch: 25it [00:07,  3.55it/s][A
All_batch: 26it [00:07,  3.44it/s][A
All_batch: 27it [00:07,  3.46it/s][A
All_batch: 28it [00:08,  3.54it/s][A
All_batch: 29it [00:08,  3.38it/s][A
All_batch: 30it [00:08,  3.50it/s][A
All_batch: 31it [00:08,  3.54it/s][AAll_batch: 31it [00:08,  3.46it/s]
[1;34mEpoch:18, Train_AUC:0.9665, Loss_rank:0.0941, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0947
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:18, topk:20, recall:0.1171, ndcg:0.0763, prec:0.0258, mrr:0.1112
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.9471, valtime:1.1998

[1;35mTrain:[0m:   0%|[33m          [0m| 19/5000 [02:38<13:32:48,  9.79s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.54it/s][A
All_batch: 2it [00:00,  3.52it/s][A
All_batch: 3it [00:00,  3.47it/s][A
All_batch: 4it [00:01,  3.55it/s][A
All_batch: 5it [00:01,  3.52it/s][A
All_batch: 6it [00:01,  3.47it/s][A
All_batch: 7it [00:01,  3.51it/s][A
All_batch: 8it [00:02,  3.51it/s][A
All_batch: 9it [00:02,  3.47it/s][A
All_batch: 10it [00:02,  3.47it/s][A
All_batch: 11it [00:03,  3.50it/s][A
All_batch: 12it [00:03,  3.40it/s][A
All_batch: 13it [00:03,  3.52it/s][A
All_batch: 14it [00:03,  3.54it/s][A
All_batch: 15it [00:04,  3.37it/s][A
All_batch: 16it [00:04,  3.47it/s][A
All_batch: 17it [00:04,  3.55it/s][A
All_batch: 18it [00:05,  3.37it/s][A
All_batch: 19it [00:05,  3.49it/s][A
All_batch: 20it [00:05,  3.57it/s][A
All_batch: 21it [00:06,  3.38it/s][A
All_batch: 22it [00:06,  3.49it/s][A
All_batch: 23it [00:06,  3.57it/s][A
All_batch: 24it [00:06,  3.37it/s][A
All_batch: 25it [00:07,  3.51it/s][A
All_batch: 26it [00:07,  3.58it/s][A
All_batch: 27it [00:07,  3.40it/s][A
All_batch: 28it [00:08,  3.51it/s][A
All_batch: 29it [00:08,  3.56it/s][A
All_batch: 30it [00:08,  3.42it/s][A
All_batch: 31it [00:08,  3.54it/s][AAll_batch: 31it [00:08,  3.49it/s]
[1;34mEpoch:19, Train_AUC:0.9647, Loss_rank:0.0945, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0952
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:19, topk:20, recall:0.1172, ndcg:0.0763, prec:0.0259, mrr:0.1113
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.8869, valtime:1.2291

[1;35mTrain:[0m:   0%|[33m          [0m| 20/5000 [02:48<13:40:45,  9.89s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  7.21it/s][A
All_batch: 2it [00:00,  6.61it/s][A
All_batch: 3it [00:00,  6.69it/s][A
All_batch: 4it [00:00,  6.53it/s][A
All_batch: 5it [00:00,  6.80it/s][A
All_batch: 6it [00:00,  6.54it/s][A
All_batch: 7it [00:01,  6.68it/s][A
All_batch: 8it [00:01,  6.50it/s][A
All_batch: 9it [00:01,  6.91it/s][A
All_batch: 10it [00:01,  6.97it/s][A
All_batch: 11it [00:01,  6.75it/s][A
All_batch: 12it [00:01,  6.83it/s][A
All_batch: 13it [00:01,  6.60it/s][A
All_batch: 14it [00:02,  6.74it/s][A
All_batch: 15it [00:02,  6.56it/s][A
All_batch: 16it [00:02,  6.99it/s][A
All_batch: 17it [00:02,  6.53it/s][A
All_batch: 18it [00:02,  6.92it/s][A
All_batch: 19it [00:02,  7.01it/s][A
All_batch: 20it [00:02,  6.72it/s][A
All_batch: 21it [00:03,  6.82it/s][A
All_batch: 22it [00:03,  6.61it/s][A
All_batch: 23it [00:03,  6.95it/s][A
All_batch: 24it [00:03,  6.56it/s][A
All_batch: 25it [00:03,  6.88it/s][A
All_batch: 26it [00:03,  6.94it/s][A
All_batch: 27it [00:04,  6.68it/s][A
All_batch: 28it [00:04,  6.82it/s][A
All_batch: 29it [00:04,  6.58it/s][A
All_batch: 30it [00:04,  6.91it/s][A
All_batch: 31it [00:04,  6.71it/s][AAll_batch: 31it [00:04,  6.75it/s]
[1;34mEpoch:20, Train_AUC:0.9657, Loss_rank:0.0943, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0949
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:20, topk:20, recall:0.1173, ndcg:0.0764, prec:0.0259, mrr:0.1114
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:4.5939, valtime:1.2087

[1;35mTrain:[0m:   0%|[33m          [0m| 21/5000 [02:54<11:58:48,  8.66s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.72it/s][A
All_batch: 2it [00:00,  3.54it/s][A
All_batch: 3it [00:00,  3.35it/s][A
All_batch: 4it [00:01,  3.51it/s][A
All_batch: 5it [00:01,  3.58it/s][A
All_batch: 6it [00:01,  3.35it/s][A
All_batch: 7it [00:02,  3.50it/s][A
All_batch: 8it [00:02,  3.59it/s][A
All_batch: 9it [00:02,  3.37it/s][A
All_batch: 10it [00:02,  3.73it/s][A
All_batch: 11it [00:03,  4.05it/s][A
All_batch: 12it [00:03,  3.96it/s][A
All_batch: 13it [00:03,  4.26it/s][A
All_batch: 14it [00:03,  4.47it/s][A
All_batch: 15it [00:03,  4.30it/s][A
All_batch: 16it [00:04,  4.59it/s][A
All_batch: 17it [00:04,  4.71it/s][A
All_batch: 18it [00:04,  4.58it/s][A
All_batch: 19it [00:04,  4.70it/s][A
All_batch: 20it [00:04,  4.83it/s][A
All_batch: 21it [00:05,  4.46it/s][A
All_batch: 22it [00:05,  4.66it/s][A
All_batch: 23it [00:05,  4.76it/s][A
All_batch: 24it [00:05,  4.47it/s][A
All_batch: 25it [00:06,  4.80it/s][A
All_batch: 26it [00:06,  4.81it/s][A
All_batch: 27it [00:06,  4.64it/s][A
All_batch: 28it [00:06,  4.77it/s][A
All_batch: 29it [00:06,  4.84it/s][A
All_batch: 30it [00:07,  4.44it/s][A
All_batch: 31it [00:07,  4.64it/s][AAll_batch: 31it [00:07,  4.24it/s]
[1;34mEpoch:21, Train_AUC:0.9659, Loss_rank:0.0931, Loss_reg:0.0004, Loss_ib:0.0002, Loss_sum:0.0938
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:21, topk:20, recall:0.1170, ndcg:0.0762, prec:0.0258, mrr:0.1112
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.3112, valtime:1.2080

[1;35mTrain:[0m:   0%|[33m          [0m| 22/5000 [03:02<11:55:07,  8.62s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.41it/s][A
All_batch: 2it [00:00,  4.89it/s][A
All_batch: 3it [00:00,  4.17it/s][A
All_batch: 4it [00:00,  3.83it/s][A
All_batch: 5it [00:01,  3.76it/s][A
All_batch: 6it [00:01,  3.58it/s][A
All_batch: 7it [00:01,  3.48it/s][A
All_batch: 8it [00:02,  3.59it/s][A
All_batch: 9it [00:02,  3.63it/s][A
All_batch: 10it [00:02,  3.40it/s][A
All_batch: 11it [00:02,  3.53it/s][A
All_batch: 12it [00:03,  3.59it/s][A
All_batch: 13it [00:03,  3.38it/s][A
All_batch: 14it [00:03,  3.48it/s][A
All_batch: 15it [00:04,  3.62it/s][A
All_batch: 16it [00:04,  3.45it/s][A
All_batch: 17it [00:04,  3.46it/s][A
All_batch: 18it [00:04,  3.63it/s][A
All_batch: 19it [00:05,  3.48it/s][A
All_batch: 20it [00:05,  3.58it/s][A
All_batch: 21it [00:05,  3.69it/s][A
All_batch: 22it [00:06,  3.53it/s][A
All_batch: 23it [00:06,  3.52it/s][A
All_batch: 24it [00:06,  3.60it/s][A
All_batch: 25it [00:06,  3.51it/s][A
All_batch: 26it [00:07,  3.52it/s][A
All_batch: 27it [00:07,  3.55it/s][A
All_batch: 28it [00:07,  3.48it/s][A
All_batch: 29it [00:08,  3.44it/s][A
All_batch: 30it [00:08,  3.47it/s][A
All_batch: 31it [00:08,  3.46it/s][AAll_batch: 31it [00:08,  3.57it/s]
[1;34mEpoch:22, Train_AUC:0.9650, Loss_rank:0.0932, Loss_reg:0.0004, Loss_ib:0.0003, Loss_sum:0.0939
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:22, topk:20, recall:0.1172, ndcg:0.0763, prec:0.0259, mrr:0.1112
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.6933, valtime:1.2026

[1;35mTrain:[0m:   0%|[33m          [0m| 23/5000 [03:12<12:26:45,  9.00s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.56it/s][A
All_batch: 2it [00:00,  3.40it/s][A
All_batch: 3it [00:00,  3.37it/s][A
All_batch: 4it [00:01,  3.52it/s][A
All_batch: 5it [00:01,  3.52it/s][A
All_batch: 6it [00:01,  3.50it/s][A
All_batch: 7it [00:01,  3.55it/s][A
All_batch: 8it [00:02,  3.49it/s][A
All_batch: 9it [00:02,  3.46it/s][A
All_batch: 10it [00:02,  3.51it/s][A
All_batch: 11it [00:03,  3.49it/s][A
All_batch: 12it [00:03,  3.43it/s][A
All_batch: 13it [00:03,  3.49it/s][A
All_batch: 14it [00:04,  3.50it/s][A
All_batch: 15it [00:04,  3.48it/s][A
All_batch: 16it [00:04,  3.53it/s][A
All_batch: 17it [00:04,  3.51it/s][A
All_batch: 18it [00:05,  3.48it/s][A
All_batch: 19it [00:05,  3.53it/s][A
All_batch: 20it [00:05,  3.51it/s][A
All_batch: 21it [00:06,  3.42it/s][A
All_batch: 22it [00:06,  3.48it/s][A
All_batch: 23it [00:06,  3.51it/s][A
All_batch: 24it [00:06,  3.40it/s][A
All_batch: 25it [00:07,  3.51it/s][A
All_batch: 26it [00:07,  3.53it/s][A
All_batch: 27it [00:07,  3.45it/s][A
All_batch: 28it [00:08,  3.49it/s][A
All_batch: 29it [00:08,  3.52it/s][A
All_batch: 30it [00:08,  3.42it/s][A
All_batch: 31it [00:08,  3.66it/s][AAll_batch: 31it [00:08,  3.50it/s]
[1;34mEpoch:23, Train_AUC:0.9661, Loss_rank:0.0927, Loss_reg:0.0004, Loss_ib:0.0003, Loss_sum:0.0935
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:23, topk:20, recall:0.1180, ndcg:0.0765, prec:0.0260, mrr:0.1113
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.8491, valtime:1.2430

[1;35mTrain:[0m:   0%|[33m          [0m| 24/5000 [03:22<12:53:43,  9.33s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  2.93it/s][A
All_batch: 2it [00:00,  3.40it/s][A
All_batch: 3it [00:00,  3.64it/s][A
All_batch: 4it [00:01,  3.31it/s][A
All_batch: 5it [00:01,  3.20it/s][A
All_batch: 6it [00:01,  3.46it/s][A
All_batch: 7it [00:02,  3.46it/s][A
All_batch: 8it [00:02,  3.66it/s][A
All_batch: 9it [00:02,  3.84it/s][A
All_batch: 10it [00:02,  3.68it/s][A
All_batch: 11it [00:03,  3.83it/s][A
All_batch: 12it [00:03,  3.97it/s][A
All_batch: 13it [00:03,  3.77it/s][A
All_batch: 14it [00:03,  3.84it/s][A
All_batch: 15it [00:04,  3.99it/s][A
All_batch: 16it [00:04,  3.86it/s][A
All_batch: 17it [00:04,  3.83it/s][A
All_batch: 18it [00:04,  3.93it/s][A
All_batch: 19it [00:05,  3.91it/s][A
All_batch: 20it [00:05,  3.83it/s][A
All_batch: 21it [00:05,  4.04it/s][A
All_batch: 22it [00:05,  3.99it/s][A
All_batch: 23it [00:06,  3.93it/s][A
All_batch: 24it [00:06,  4.06it/s][A
All_batch: 25it [00:06,  4.02it/s][A
All_batch: 26it [00:06,  3.91it/s][A
All_batch: 27it [00:07,  4.02it/s][A
All_batch: 28it [00:07,  4.01it/s][A
All_batch: 29it [00:07,  3.86it/s][A
All_batch: 30it [00:07,  4.06it/s][A
All_batch: 31it [00:08,  4.05it/s][AAll_batch: 31it [00:08,  3.82it/s]
[1;34mEpoch:24, Train_AUC:0.9641, Loss_rank:0.0942, Loss_reg:0.0005, Loss_ib:0.0003, Loss_sum:0.0949
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:24, topk:20, recall:0.1174, ndcg:0.0765, prec:0.0259, mrr:0.1112
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:8.1092, valtime:1.2061

[1;35mTrain:[0m:   0%|[33m          [0m| 25/5000 [03:32<12:53:13,  9.33s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.34it/s][A
All_batch: 2it [00:00,  3.74it/s][A
All_batch: 3it [00:00,  4.04it/s][A
All_batch: 4it [00:01,  3.76it/s][A
All_batch: 5it [00:01,  3.81it/s][A
All_batch: 6it [00:01,  4.02it/s][A
All_batch: 7it [00:01,  3.86it/s][A
All_batch: 8it [00:02,  3.86it/s][A
All_batch: 9it [00:02,  3.95it/s][A
All_batch: 10it [00:02,  3.92it/s][A
All_batch: 11it [00:02,  3.86it/s][A
All_batch: 12it [00:03,  4.01it/s][A
All_batch: 13it [00:03,  3.99it/s][A
All_batch: 14it [00:03,  3.90it/s][A
All_batch: 15it [00:03,  4.06it/s][A
All_batch: 16it [00:04,  4.03it/s][A
All_batch: 17it [00:04,  3.95it/s][A
All_batch: 18it [00:04,  4.16it/s][A
All_batch: 19it [00:04,  4.04it/s][A
All_batch: 20it [00:05,  3.98it/s][A
All_batch: 21it [00:05,  4.16it/s][A
All_batch: 22it [00:05,  4.03it/s][A
All_batch: 23it [00:05,  4.02it/s][A
All_batch: 24it [00:06,  4.16it/s][A
All_batch: 25it [00:06,  4.07it/s][A
All_batch: 26it [00:06,  4.02it/s][A
All_batch: 27it [00:06,  4.10it/s][A
All_batch: 28it [00:07,  3.99it/s][A
All_batch: 29it [00:07,  3.86it/s][A
All_batch: 30it [00:07,  3.97it/s][A
All_batch: 31it [00:07,  3.96it/s][AAll_batch: 31it [00:07,  3.97it/s]
[1;34mEpoch:25, Train_AUC:0.9655, Loss_rank:0.0906, Loss_reg:0.0005, Loss_ib:0.0003, Loss_sum:0.0914
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:25, topk:20, recall:0.1164, ndcg:0.0762, prec:0.0257, mrr:0.1115
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8073, valtime:1.2282

[1;35mTrain:[0m:   1%|[33m          [0m| 26/5000 [03:41<12:45:51,  9.24s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.52it/s][A
All_batch: 2it [00:00,  3.76it/s][A
All_batch: 3it [00:00,  4.40it/s][A
All_batch: 4it [00:01,  3.96it/s][A
All_batch: 5it [00:01,  4.05it/s][A
All_batch: 6it [00:01,  4.18it/s][A
All_batch: 7it [00:01,  3.91it/s][A
All_batch: 8it [00:02,  3.90it/s][A
All_batch: 9it [00:02,  4.07it/s][A
All_batch: 10it [00:02,  3.90it/s][A
All_batch: 11it [00:02,  3.91it/s][A
All_batch: 12it [00:03,  4.00it/s][A
All_batch: 13it [00:03,  3.94it/s][A
All_batch: 14it [00:03,  3.90it/s][A
All_batch: 15it [00:03,  4.01it/s][A
All_batch: 16it [00:04,  4.00it/s][A
All_batch: 17it [00:04,  3.89it/s][A
All_batch: 18it [00:04,  4.05it/s][A
All_batch: 19it [00:04,  4.05it/s][A
All_batch: 20it [00:05,  3.98it/s][A
All_batch: 21it [00:05,  4.10it/s][A
All_batch: 22it [00:05,  4.06it/s][A
All_batch: 23it [00:05,  3.94it/s][A
All_batch: 24it [00:05,  4.14it/s][A
All_batch: 25it [00:06,  4.05it/s][A
All_batch: 26it [00:06,  3.93it/s][A
All_batch: 27it [00:06,  4.08it/s][A
All_batch: 28it [00:07,  3.91it/s][A
All_batch: 29it [00:07,  3.91it/s][A
All_batch: 30it [00:07,  4.14it/s][A
All_batch: 31it [00:07,  3.96it/s][AAll_batch: 31it [00:07,  3.99it/s]
[1;34mEpoch:26, Train_AUC:0.9664, Loss_rank:0.0907, Loss_reg:0.0005, Loss_ib:0.0003, Loss_sum:0.0915
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:26, topk:20, recall:0.1165, ndcg:0.0762, prec:0.0257, mrr:0.1113
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.7690, valtime:1.2012

[1;35mTrain:[0m:   1%|[33m          [0m| 27/5000 [03:50<12:39:02,  9.16s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.79it/s][A
All_batch: 2it [00:00,  4.24it/s][A
All_batch: 3it [00:00,  4.01it/s][A
All_batch: 4it [00:01,  3.96it/s][A
All_batch: 5it [00:01,  4.20it/s][A
All_batch: 6it [00:01,  4.03it/s][A
All_batch: 7it [00:01,  3.99it/s][A
All_batch: 8it [00:01,  4.16it/s][A
All_batch: 9it [00:02,  4.04it/s][A
All_batch: 10it [00:02,  4.02it/s][A
All_batch: 11it [00:02,  4.17it/s][A
All_batch: 12it [00:02,  4.03it/s][A
All_batch: 13it [00:03,  3.97it/s][A
All_batch: 14it [00:03,  4.00it/s][A
All_batch: 15it [00:03,  3.90it/s][A
All_batch: 16it [00:04,  3.83it/s][A
All_batch: 17it [00:04,  3.98it/s][A
All_batch: 18it [00:04,  3.88it/s][A
All_batch: 19it [00:04,  3.81it/s][A
All_batch: 20it [00:05,  4.00it/s][A
All_batch: 21it [00:05,  3.93it/s][A
All_batch: 22it [00:05,  3.86it/s][A
All_batch: 23it [00:05,  3.98it/s][A
All_batch: 24it [00:06,  4.00it/s][A
All_batch: 25it [00:06,  3.91it/s][A
All_batch: 26it [00:06,  4.02it/s][A
All_batch: 27it [00:06,  4.03it/s][A
All_batch: 28it [00:07,  3.87it/s][A
All_batch: 29it [00:07,  4.06it/s][A
All_batch: 30it [00:07,  4.05it/s][A
All_batch: 31it [00:07,  3.87it/s][AAll_batch: 31it [00:07,  3.97it/s]
[1;34mEpoch:27, Train_AUC:0.9652, Loss_rank:0.0930, Loss_reg:0.0005, Loss_ib:0.0003, Loss_sum:0.0938
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:27, topk:20, recall:0.1164, ndcg:0.0762, prec:0.0257, mrr:0.1115
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8013, valtime:1.2338

[1;35mTrain:[0m:   1%|[33m          [0m| 28/5000 [03:59<12:35:50,  9.12s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 3it [00:00, 20.26it/s][A
All_batch: 6it [00:00, 20.40it/s][A
All_batch: 9it [00:00, 20.46it/s][A
All_batch: 12it [00:00, 20.32it/s][A
All_batch: 15it [00:00, 19.42it/s][A
All_batch: 17it [00:00, 19.31it/s][A
All_batch: 19it [00:00, 19.26it/s][A
All_batch: 21it [00:01, 19.20it/s][A
All_batch: 23it [00:01, 19.15it/s][A
All_batch: 25it [00:01, 19.19it/s][A
All_batch: 27it [00:01, 19.10it/s][A
All_batch: 30it [00:01, 19.62it/s][AAll_batch: 31it [00:01, 19.61it/s]
[1;34mEpoch:28, Train_AUC:0.9643, Loss_rank:0.0913, Loss_reg:0.0005, Loss_ib:0.0004, Loss_sum:0.0921
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:28, topk:20, recall:0.1164, ndcg:0.0769, prec:0.0257, mrr:0.1137
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:1.5814, valtime:1.2183

[1;35mTrain:[0m:   1%|[33m          [0m| 29/5000 [04:01<9:58:34,  7.22s/it] 
All_batch: 0it [00:00, ?it/s][A
All_batch: 2it [00:00, 19.98it/s][A
All_batch: 5it [00:00, 20.38it/s][A
All_batch: 8it [00:00, 20.50it/s][A
All_batch: 11it [00:00, 20.60it/s][A
All_batch: 14it [00:00, 20.50it/s][A
All_batch: 17it [00:00, 20.40it/s][A
All_batch: 20it [00:00, 20.48it/s][A
All_batch: 23it [00:01, 20.47it/s][A
All_batch: 26it [00:01, 20.58it/s][A
All_batch: 29it [00:01, 20.56it/s][AAll_batch: 31it [00:01, 20.51it/s]
[1;34mEpoch:29, Train_AUC:0.9660, Loss_rank:0.0897, Loss_reg:0.0005, Loss_ib:0.0004, Loss_sum:0.0905
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:29, topk:20, recall:0.1168, ndcg:0.0772, prec:0.0257, mrr:0.1143
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:1.5119, valtime:1.2198

[1;35mTrain:[0m:   1%|[33m          [0m| 30/5000 [04:04<8:06:48,  5.88s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  6.45it/s][A
All_batch: 2it [00:00,  4.75it/s][A
All_batch: 3it [00:00,  4.13it/s][A
All_batch: 4it [00:00,  4.15it/s][A
All_batch: 5it [00:01,  4.14it/s][A
All_batch: 6it [00:01,  3.83it/s][A
All_batch: 7it [00:01,  3.93it/s][A
All_batch: 8it [00:01,  4.06it/s][A
All_batch: 9it [00:02,  3.84it/s][A
All_batch: 10it [00:02,  3.87it/s][A
All_batch: 11it [00:02,  4.02it/s][A
All_batch: 12it [00:02,  3.89it/s][A
All_batch: 13it [00:03,  3.87it/s][A
All_batch: 14it [00:03,  3.96it/s][A
All_batch: 15it [00:03,  3.94it/s][A
All_batch: 16it [00:04,  3.90it/s][A
All_batch: 17it [00:04,  4.03it/s][A
All_batch: 18it [00:04,  4.00it/s][A
All_batch: 19it [00:04,  3.92it/s][A
All_batch: 20it [00:04,  4.09it/s][A
All_batch: 21it [00:05,  4.07it/s][A
All_batch: 22it [00:05,  3.99it/s][A
All_batch: 23it [00:05,  4.18it/s][A
All_batch: 24it [00:05,  4.05it/s][A
All_batch: 25it [00:06,  4.01it/s][A
All_batch: 26it [00:06,  4.19it/s][A
All_batch: 27it [00:06,  4.03it/s][A
All_batch: 28it [00:06,  4.01it/s][A
All_batch: 29it [00:07,  4.19it/s][A
All_batch: 30it [00:07,  4.00it/s][A
All_batch: 31it [00:07,  3.92it/s][AAll_batch: 31it [00:07,  4.02it/s]
[1;34mEpoch:30, Train_AUC:0.9659, Loss_rank:0.0886, Loss_reg:0.0005, Loss_ib:0.0004, Loss_sum:0.0895
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:30, topk:20, recall:0.1161, ndcg:0.0769, prec:0.0256, mrr:0.1141
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.7100, valtime:1.2017

[1;35mTrain:[0m:   1%|[33m          [0m| 31/5000 [04:13<9:22:06,  6.79s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.19it/s][A
All_batch: 2it [00:00,  4.05it/s][A
All_batch: 3it [00:00,  3.98it/s][A
All_batch: 4it [00:00,  4.18it/s][A
All_batch: 5it [00:01,  4.02it/s][A
All_batch: 6it [00:01,  4.00it/s][A
All_batch: 7it [00:01,  4.14it/s][A
All_batch: 8it [00:01,  4.01it/s][A
All_batch: 9it [00:02,  3.98it/s][A
All_batch: 10it [00:02,  4.08it/s][A
All_batch: 11it [00:02,  4.00it/s][A
All_batch: 12it [00:02,  3.93it/s][A
All_batch: 13it [00:03,  4.05it/s][A
All_batch: 14it [00:03,  4.03it/s][A
All_batch: 15it [00:03,  3.91it/s][A
All_batch: 16it [00:03,  4.05it/s][A
All_batch: 17it [00:04,  4.05it/s][A
All_batch: 18it [00:04,  3.85it/s][A
All_batch: 19it [00:04,  3.94it/s][A
All_batch: 20it [00:05,  3.93it/s][A
All_batch: 21it [00:05,  3.74it/s][A
All_batch: 22it [00:05,  3.85it/s][A
All_batch: 23it [00:05,  4.00it/s][A
All_batch: 24it [00:06,  3.82it/s][A
All_batch: 25it [00:06,  3.85it/s][A
All_batch: 26it [00:06,  4.03it/s][A
All_batch: 27it [00:06,  3.89it/s][A
All_batch: 28it [00:07,  3.88it/s][A
All_batch: 29it [00:07,  4.00it/s][A
All_batch: 30it [00:07,  3.94it/s][A
All_batch: 31it [00:07,  3.90it/s][AAll_batch: 31it [00:07,  3.96it/s]
[1;34mEpoch:31, Train_AUC:0.9680, Loss_rank:0.0867, Loss_reg:0.0005, Loss_ib:0.0004, Loss_sum:0.0876
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:31, topk:20, recall:0.1162, ndcg:0.0771, prec:0.0256, mrr:0.1144
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8310, valtime:1.2075

[1;35mTrain:[0m:   1%|[33m          [0m| 32/5000 [04:22<10:17:55,  7.46s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.56it/s][A
All_batch: 2it [00:00,  4.26it/s][A
All_batch: 3it [00:00,  3.89it/s][A
All_batch: 4it [00:00,  4.16it/s][A
All_batch: 5it [00:01,  4.13it/s][A
All_batch: 6it [00:01,  3.89it/s][A
All_batch: 7it [00:01,  4.01it/s][A
All_batch: 8it [00:01,  3.96it/s][A
All_batch: 9it [00:02,  3.75it/s][A
All_batch: 10it [00:02,  3.87it/s][A
All_batch: 11it [00:02,  4.00it/s][A
All_batch: 12it [00:03,  3.82it/s][A
All_batch: 13it [00:03,  3.91it/s][A
All_batch: 14it [00:03,  4.07it/s][A
All_batch: 15it [00:03,  3.90it/s][A
All_batch: 16it [00:04,  3.92it/s][A
All_batch: 17it [00:04,  4.04it/s][A
All_batch: 18it [00:04,  3.93it/s][A
All_batch: 19it [00:04,  3.91it/s][A
All_batch: 20it [00:05,  4.18it/s][A
All_batch: 21it [00:05,  3.99it/s][A
All_batch: 22it [00:05,  4.01it/s][A
All_batch: 23it [00:05,  4.16it/s][A
All_batch: 24it [00:06,  3.98it/s][A
All_batch: 25it [00:06,  3.98it/s][A
All_batch: 26it [00:06,  4.17it/s][A
All_batch: 27it [00:06,  4.00it/s][A
All_batch: 28it [00:07,  3.99it/s][A
All_batch: 29it [00:07,  4.12it/s][A
All_batch: 30it [00:07,  3.96it/s][A
All_batch: 31it [00:07,  3.81it/s][AAll_batch: 31it [00:07,  3.98it/s]
[1;34mEpoch:32, Train_AUC:0.9656, Loss_rank:0.0910, Loss_reg:0.0005, Loss_ib:0.0005, Loss_sum:0.0919
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:32, topk:20, recall:0.1168, ndcg:0.0773, prec:0.0257, mrr:0.1145
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.7975, valtime:1.2271

[1;35mTrain:[0m:   1%|[33m          [0m| 33/5000 [04:31<10:56:35,  7.93s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.30it/s][A
All_batch: 2it [00:00,  4.28it/s][A
All_batch: 3it [00:00,  3.90it/s][A
All_batch: 4it [00:00,  4.14it/s][A
All_batch: 5it [00:01,  4.11it/s][A
All_batch: 6it [00:01,  3.88it/s][A
All_batch: 7it [00:01,  3.99it/s][A
All_batch: 8it [00:01,  4.08it/s][A
All_batch: 9it [00:02,  3.86it/s][A
All_batch: 10it [00:02,  3.89it/s][A
All_batch: 11it [00:02,  4.04it/s][A
All_batch: 12it [00:03,  3.90it/s][A
All_batch: 13it [00:03,  3.85it/s][A
All_batch: 14it [00:03,  4.00it/s][A
All_batch: 15it [00:03,  3.95it/s][A
All_batch: 16it [00:04,  3.96it/s][A
All_batch: 17it [00:04,  4.10it/s][A
All_batch: 18it [00:04,  3.93it/s][A
All_batch: 19it [00:04,  3.88it/s][A
All_batch: 20it [00:05,  3.94it/s][A
All_batch: 21it [00:05,  3.86it/s][A
All_batch: 22it [00:05,  3.80it/s][A
All_batch: 23it [00:05,  3.95it/s][A
All_batch: 24it [00:06,  3.94it/s][A
All_batch: 25it [00:06,  3.83it/s][A
All_batch: 26it [00:06,  4.01it/s][A
All_batch: 27it [00:06,  4.02it/s][A
All_batch: 28it [00:07,  3.90it/s][A
All_batch: 29it [00:07,  4.05it/s][A
All_batch: 30it [00:07,  4.04it/s][A
All_batch: 31it [00:07,  3.87it/s][AAll_batch: 31it [00:07,  3.95it/s]
[1;34mEpoch:33, Train_AUC:0.9662, Loss_rank:0.0879, Loss_reg:0.0005, Loss_ib:0.0005, Loss_sum:0.0889
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:33, topk:20, recall:0.1166, ndcg:0.0771, prec:0.0256, mrr:0.1139
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8442, valtime:1.1859

[1;35mTrain:[0m:   1%|[33m          [0m| 34/5000 [04:40<11:23:44,  8.26s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.90it/s][A
All_batch: 2it [00:00,  4.10it/s][A
All_batch: 3it [00:00,  3.94it/s][A
All_batch: 4it [00:01,  3.91it/s][A
All_batch: 5it [00:01,  4.05it/s][A
All_batch: 6it [00:01,  3.93it/s][A
All_batch: 7it [00:01,  3.79it/s][A
All_batch: 8it [00:02,  3.91it/s][A
All_batch: 9it [00:02,  3.89it/s][A
All_batch: 10it [00:02,  3.76it/s][A
All_batch: 11it [00:02,  3.95it/s][A
All_batch: 12it [00:03,  3.97it/s][A
All_batch: 13it [00:03,  3.78it/s][A
All_batch: 14it [00:03,  3.98it/s][A
All_batch: 15it [00:03,  4.00it/s][A
All_batch: 16it [00:04,  3.81it/s][A
All_batch: 17it [00:04,  3.93it/s][A
All_batch: 18it [00:04,  4.07it/s][A
All_batch: 19it [00:04,  3.87it/s][A
All_batch: 20it [00:05,  4.01it/s][A
All_batch: 21it [00:05,  4.16it/s][A
All_batch: 22it [00:05,  3.93it/s][A
All_batch: 23it [00:05,  4.02it/s][A
All_batch: 24it [00:06,  4.12it/s][A
All_batch: 25it [00:06,  3.95it/s][A
All_batch: 26it [00:06,  4.01it/s][A
All_batch: 27it [00:06,  4.19it/s][A
All_batch: 28it [00:07,  3.95it/s][A
All_batch: 29it [00:07,  3.97it/s][A
All_batch: 30it [00:07,  4.09it/s][A
All_batch: 31it [00:07,  3.87it/s][AAll_batch: 31it [00:07,  3.95it/s]
[1;34mEpoch:34, Train_AUC:0.9654, Loss_rank:0.0906, Loss_reg:0.0005, Loss_ib:0.0005, Loss_sum:0.0916
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:34, topk:20, recall:0.1170, ndcg:0.0770, prec:0.0257, mrr:0.1137
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8457, valtime:1.2188

[1;35mTrain:[0m:   1%|[33m          [0m| 35/5000 [04:49<11:43:33,  8.50s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.49it/s][A
All_batch: 2it [00:00,  4.10it/s][A
All_batch: 3it [00:00,  4.12it/s][A
All_batch: 4it [00:01,  3.88it/s][A
All_batch: 5it [00:01,  4.09it/s][A
All_batch: 6it [00:01,  4.07it/s][A
All_batch: 7it [00:01,  3.88it/s][A
All_batch: 8it [00:01,  4.07it/s][A
All_batch: 9it [00:02,  4.06it/s][A
All_batch: 10it [00:02,  3.88it/s][A
All_batch: 11it [00:02,  4.01it/s][A
All_batch: 12it [00:02,  4.06it/s][A
All_batch: 13it [00:03,  3.86it/s][A
All_batch: 14it [00:03,  3.89it/s][A
All_batch: 15it [00:03,  4.07it/s][A
All_batch: 16it [00:04,  3.90it/s][A
All_batch: 17it [00:04,  3.85it/s][A
All_batch: 18it [00:04,  3.93it/s][A
All_batch: 19it [00:04,  3.85it/s][A
All_batch: 20it [00:05,  3.83it/s][A
All_batch: 21it [00:05,  3.95it/s][A
All_batch: 22it [00:05,  3.86it/s][A
All_batch: 23it [00:05,  3.87it/s][A
All_batch: 24it [00:06,  3.99it/s][A
All_batch: 25it [00:06,  3.90it/s][A
All_batch: 26it [00:06,  3.88it/s][A
All_batch: 27it [00:06,  4.00it/s][A
All_batch: 28it [00:07,  3.98it/s][A
All_batch: 29it [00:07,  3.91it/s][A
All_batch: 30it [00:07,  4.61it/s][A
All_batch: 31it [00:07,  4.60it/s][AAll_batch: 31it [00:07,  4.02it/s]
[1;34mEpoch:35, Train_AUC:0.9661, Loss_rank:0.0900, Loss_reg:0.0005, Loss_ib:0.0005, Loss_sum:0.0910
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:35, topk:20, recall:0.1169, ndcg:0.0769, prec:0.0256, mrr:0.1136
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.7070, valtime:1.2598

[1;35mTrain:[0m:   1%|[33m          [0m| 36/5000 [04:58<11:54:57,  8.64s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  5.15it/s][A
All_batch: 2it [00:00,  6.42it/s][A
All_batch: 3it [00:00,  5.41it/s][A
All_batch: 4it [00:00,  5.41it/s][A
All_batch: 5it [00:00,  5.96it/s][A
All_batch: 6it [00:01,  5.40it/s][A
All_batch: 7it [00:01,  5.62it/s][A
All_batch: 8it [00:01,  5.84it/s][A
All_batch: 9it [00:01,  5.31it/s][A
All_batch: 10it [00:01,  5.58it/s][A
All_batch: 11it [00:01,  5.50it/s][A
All_batch: 12it [00:02,  5.12it/s][A
All_batch: 13it [00:02,  5.27it/s][A
All_batch: 14it [00:02,  5.48it/s][A
All_batch: 15it [00:02,  5.19it/s][A
All_batch: 16it [00:02,  5.56it/s][A
All_batch: 17it [00:03,  5.56it/s][A
All_batch: 18it [00:03,  5.18it/s][A
All_batch: 19it [00:03,  5.55it/s][A
All_batch: 20it [00:03,  5.55it/s][A
All_batch: 21it [00:03,  5.23it/s][A
All_batch: 22it [00:04,  5.61it/s][A
All_batch: 23it [00:04,  5.57it/s][A
All_batch: 24it [00:04,  5.22it/s][A
All_batch: 25it [00:04,  5.42it/s][A
All_batch: 26it [00:04,  5.66it/s][A
All_batch: 27it [00:04,  5.29it/s][A
All_batch: 28it [00:05,  4.95it/s][A
All_batch: 29it [00:05,  4.66it/s][A
All_batch: 30it [00:05,  5.35it/s][A
All_batch: 31it [00:05,  5.94it/s][AAll_batch: 31it [00:05,  5.46it/s]
[1;34mEpoch:36, Train_AUC:0.9662, Loss_rank:0.0862, Loss_reg:0.0005, Loss_ib:0.0006, Loss_sum:0.0872
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:36, topk:20, recall:0.1176, ndcg:0.0773, prec:0.0257, mrr:0.1138
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:5.6811, valtime:1.2460

[1;35mTrain:[0m:   1%|[33m          [0m| 37/5000 [05:05<11:12:16,  8.13s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  7.71it/s][A
All_batch: 2it [00:00,  7.95it/s][A
All_batch: 3it [00:00,  8.05it/s][A
All_batch: 4it [00:00,  8.09it/s][A
All_batch: 5it [00:00,  8.13it/s][A
All_batch: 6it [00:00,  8.03it/s][A
All_batch: 7it [00:00,  7.80it/s][A
All_batch: 8it [00:01,  7.77it/s][A
All_batch: 9it [00:01,  7.74it/s][A
All_batch: 10it [00:01,  7.78it/s][A
All_batch: 11it [00:01,  7.72it/s][A
All_batch: 12it [00:01,  7.69it/s][A
All_batch: 13it [00:01,  7.78it/s][A
All_batch: 14it [00:01,  7.91it/s][A
All_batch: 15it [00:01,  7.99it/s][A
All_batch: 16it [00:02,  8.06it/s][A
All_batch: 17it [00:02,  8.08it/s][A
All_batch: 18it [00:02,  8.11it/s][A
All_batch: 19it [00:02,  8.05it/s][A
All_batch: 20it [00:02,  7.98it/s][A
All_batch: 21it [00:02,  7.99it/s][A
All_batch: 22it [00:02,  8.05it/s][A
All_batch: 23it [00:02,  7.97it/s][A
All_batch: 24it [00:03,  8.10it/s][A
All_batch: 25it [00:03,  8.05it/s][A
All_batch: 26it [00:03,  8.11it/s][A
All_batch: 27it [00:03,  8.11it/s][A
All_batch: 28it [00:03,  8.15it/s][A
All_batch: 29it [00:03,  8.14it/s][A
All_batch: 30it [00:03,  8.15it/s][A
All_batch: 31it [00:03,  8.16it/s][AAll_batch: 31it [00:03,  8.00it/s]
[1;34mEpoch:37, Train_AUC:0.9665, Loss_rank:0.0879, Loss_reg:0.0005, Loss_ib:0.0006, Loss_sum:0.0890
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:37, topk:20, recall:0.1177, ndcg:0.0774, prec:0.0258, mrr:0.1140
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:3.8767, valtime:1.1901

[1;35mTrain:[0m:   1%|[33m          [0m| 38/5000 [05:10<9:56:12,  7.21s/it] 
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.10it/s][A
All_batch: 2it [00:00,  3.78it/s][A
All_batch: 3it [00:00,  4.11it/s][A
All_batch: 4it [00:00,  4.08it/s][A
All_batch: 5it [00:01,  3.86it/s][A
All_batch: 6it [00:01,  4.00it/s][A
All_batch: 7it [00:01,  4.00it/s][A
All_batch: 8it [00:02,  3.76it/s][A
All_batch: 9it [00:02,  3.77it/s][A
All_batch: 10it [00:02,  3.94it/s][A
All_batch: 11it [00:02,  3.81it/s][A
All_batch: 12it [00:03,  3.82it/s][A
All_batch: 13it [00:03,  3.94it/s][A
All_batch: 14it [00:03,  3.88it/s][A
All_batch: 15it [00:03,  3.89it/s][A
All_batch: 16it [00:04,  4.06it/s][A
All_batch: 17it [00:04,  3.96it/s][A
All_batch: 18it [00:04,  3.94it/s][A
All_batch: 19it [00:04,  4.04it/s][A
All_batch: 20it [00:05,  3.97it/s][A
All_batch: 21it [00:05,  3.90it/s][A
All_batch: 22it [00:05,  4.02it/s][A
All_batch: 23it [00:05,  4.00it/s][A
All_batch: 24it [00:06,  3.90it/s][A
All_batch: 25it [00:06,  4.09it/s][A
All_batch: 26it [00:06,  4.04it/s][A
All_batch: 27it [00:06,  3.91it/s][A
All_batch: 28it [00:07,  4.03it/s][A
All_batch: 29it [00:07,  4.02it/s][A
All_batch: 30it [00:07,  3.84it/s][A
All_batch: 31it [00:07,  3.94it/s][AAll_batch: 31it [00:07,  3.94it/s]
[1;34mEpoch:38, Train_AUC:0.9673, Loss_rank:0.0852, Loss_reg:0.0005, Loss_ib:0.0006, Loss_sum:0.0863
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:38, topk:20, recall:0.1172, ndcg:0.0770, prec:0.0257, mrr:0.1133
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8646, valtime:1.2258

[1;35mTrain:[0m:   1%|[33m          [0m| 39/5000 [05:19<10:42:45,  7.77s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.29it/s][A
All_batch: 2it [00:00,  3.71it/s][A
All_batch: 3it [00:00,  3.83it/s][A
All_batch: 4it [00:01,  4.08it/s][A
All_batch: 5it [00:01,  3.88it/s][A
All_batch: 6it [00:01,  3.87it/s][A
All_batch: 7it [00:01,  4.00it/s][A
All_batch: 8it [00:02,  3.95it/s][A
All_batch: 9it [00:02,  3.92it/s][A
All_batch: 10it [00:02,  4.04it/s][A
All_batch: 11it [00:02,  4.00it/s][A
All_batch: 12it [00:03,  3.90it/s][A
All_batch: 13it [00:03,  4.02it/s][A
All_batch: 14it [00:03,  4.03it/s][A
All_batch: 15it [00:03,  3.96it/s][A
All_batch: 16it [00:03,  4.19it/s][A
All_batch: 17it [00:04,  4.07it/s][A
All_batch: 18it [00:04,  4.01it/s][A
All_batch: 19it [00:04,  4.11it/s][A
All_batch: 20it [00:05,  3.90it/s][A
All_batch: 21it [00:05,  3.85it/s][A
All_batch: 22it [00:05,  4.06it/s][A
All_batch: 23it [00:05,  3.92it/s][A
All_batch: 24it [00:06,  3.95it/s][A
All_batch: 25it [00:06,  4.11it/s][A
All_batch: 26it [00:06,  4.01it/s][A
All_batch: 27it [00:06,  3.98it/s][A
All_batch: 28it [00:06,  4.14it/s][A
All_batch: 29it [00:07,  3.98it/s][A
All_batch: 30it [00:07,  3.91it/s][A
All_batch: 31it [00:07,  4.03it/s][AAll_batch: 31it [00:07,  3.99it/s]
[1;34mEpoch:39, Train_AUC:0.9664, Loss_rank:0.0882, Loss_reg:0.0005, Loss_ib:0.0007, Loss_sum:0.0893
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:39, topk:20, recall:0.1164, ndcg:0.0769, prec:0.0256, mrr:0.1134
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.7688, valtime:1.1763

[1;35mTrain:[0m:   1%|[33m          [0m| 40/5000 [05:28<11:11:40,  8.13s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.80it/s][A
All_batch: 2it [00:00,  3.68it/s][A
All_batch: 3it [00:00,  3.96it/s][A
All_batch: 4it [00:01,  3.98it/s][A
All_batch: 5it [00:01,  3.81it/s][A
All_batch: 6it [00:01,  4.04it/s][A
All_batch: 7it [00:01,  3.98it/s][A
All_batch: 8it [00:02,  3.78it/s][A
All_batch: 9it [00:02,  3.94it/s][A
All_batch: 10it [00:02,  3.92it/s][A
All_batch: 11it [00:02,  3.77it/s][A
All_batch: 12it [00:03,  3.98it/s][A
All_batch: 13it [00:03,  4.00it/s][A
All_batch: 14it [00:03,  3.80it/s][A
All_batch: 15it [00:03,  3.92it/s][A
All_batch: 16it [00:04,  4.01it/s][A
All_batch: 17it [00:04,  3.83it/s][A
All_batch: 18it [00:04,  3.87it/s][A
All_batch: 19it [00:04,  4.03it/s][A
All_batch: 20it [00:05,  3.89it/s][A
All_batch: 21it [00:05,  3.86it/s][A
All_batch: 22it [00:05,  4.02it/s][A
All_batch: 23it [00:05,  3.93it/s][A
All_batch: 24it [00:06,  3.92it/s][A
All_batch: 25it [00:06,  4.08it/s][A
All_batch: 26it [00:06,  3.96it/s][A
All_batch: 27it [00:06,  3.95it/s][A
All_batch: 28it [00:07,  4.04it/s][A
All_batch: 29it [00:07,  3.97it/s][A
All_batch: 30it [00:07,  3.87it/s][A
All_batch: 31it [00:07,  3.93it/s][AAll_batch: 31it [00:07,  3.92it/s]
[1;34mEpoch:40, Train_AUC:0.9647, Loss_rank:0.0892, Loss_reg:0.0005, Loss_ib:0.0007, Loss_sum:0.0904
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:40, topk:20, recall:0.1173, ndcg:0.0771, prec:0.0257, mrr:0.1136
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.9004, valtime:1.2030

[1;35mTrain:[0m:   1%|[33m          [0m| 41/5000 [05:37<11:35:48,  8.42s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.85it/s][A
All_batch: 2it [00:00,  3.62it/s][A
All_batch: 3it [00:00,  3.94it/s][A
All_batch: 4it [00:01,  4.02it/s][A
All_batch: 5it [00:01,  3.80it/s][A
All_batch: 6it [00:01,  3.95it/s][A
All_batch: 7it [00:01,  4.07it/s][A
All_batch: 8it [00:02,  3.85it/s][A
All_batch: 9it [00:02,  3.90it/s][A
All_batch: 10it [00:02,  4.06it/s][A
All_batch: 11it [00:02,  3.90it/s][A
All_batch: 12it [00:03,  3.89it/s][A
All_batch: 13it [00:03,  4.00it/s][A
All_batch: 14it [00:03,  3.94it/s][A
All_batch: 15it [00:03,  3.90it/s][A
All_batch: 16it [00:04,  4.02it/s][A
All_batch: 17it [00:04,  3.99it/s][A
All_batch: 18it [00:04,  3.91it/s][A
All_batch: 19it [00:04,  3.98it/s][A
All_batch: 20it [00:05,  3.94it/s][A
All_batch: 21it [00:05,  3.81it/s][A
All_batch: 22it [00:05,  3.90it/s][A
All_batch: 23it [00:05,  3.91it/s][A
All_batch: 24it [00:06,  3.77it/s][A
All_batch: 25it [00:06,  3.99it/s][A
All_batch: 26it [00:06,  3.97it/s][A
All_batch: 27it [00:06,  3.82it/s][A
All_batch: 28it [00:07,  4.03it/s][A
All_batch: 29it [00:07,  4.03it/s][A
All_batch: 30it [00:07,  3.86it/s][A
All_batch: 31it [00:07,  4.07it/s][AAll_batch: 31it [00:07,  3.94it/s]
[1;34mEpoch:41, Train_AUC:0.9665, Loss_rank:0.0872, Loss_reg:0.0005, Loss_ib:0.0007, Loss_sum:0.0884
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:41, topk:20, recall:0.1178, ndcg:0.0773, prec:0.0259, mrr:0.1135
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8697, valtime:1.2031

[1;35mTrain:[0m:   1%|[33m          [0m| 42/5000 [05:46<11:51:53,  8.61s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  5.86it/s][A
All_batch: 2it [00:00,  5.06it/s][A
All_batch: 3it [00:00,  5.68it/s][A
All_batch: 4it [00:00,  5.62it/s][A
All_batch: 5it [00:00,  5.20it/s][A
All_batch: 6it [00:01,  5.64it/s][A
All_batch: 7it [00:01,  5.60it/s][A
All_batch: 8it [00:01,  5.24it/s][A
All_batch: 9it [00:01,  5.62it/s][A
All_batch: 10it [00:01,  5.46it/s][A
All_batch: 11it [00:02,  5.10it/s][A
All_batch: 12it [00:02,  5.48it/s][A
All_batch: 13it [00:02,  5.47it/s][A
All_batch: 14it [00:02,  5.47it/s][A
All_batch: 15it [00:02,  5.91it/s][A
All_batch: 16it [00:02,  5.42it/s][A
All_batch: 17it [00:03,  5.49it/s][A
All_batch: 18it [00:03,  5.88it/s][A
All_batch: 19it [00:03,  5.38it/s][A
All_batch: 20it [00:03,  5.72it/s][A
All_batch: 21it [00:03,  5.66it/s][A
All_batch: 22it [00:04,  5.30it/s][A
All_batch: 23it [00:04,  5.67it/s][A
All_batch: 24it [00:04,  5.61it/s][A
All_batch: 25it [00:04,  5.27it/s][A
All_batch: 26it [00:04,  5.63it/s][A
All_batch: 27it [00:04,  5.59it/s][A
All_batch: 28it [00:05,  5.23it/s][A
All_batch: 29it [00:05,  5.44it/s][A
All_batch: 30it [00:05,  5.67it/s][A
All_batch: 31it [00:05,  5.29it/s][AAll_batch: 31it [00:05,  5.48it/s]
[1;34mEpoch:42, Train_AUC:0.9669, Loss_rank:0.0858, Loss_reg:0.0005, Loss_ib:0.0008, Loss_sum:0.0871
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:42, topk:20, recall:0.1170, ndcg:0.0771, prec:0.0258, mrr:0.1134
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:5.6586, valtime:1.2211

[1;35mTrain:[0m:   1%|[33m          [0m| 43/5000 [05:53<11:08:44,  8.09s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.26it/s][A
All_batch: 2it [00:00,  4.12it/s][A
All_batch: 3it [00:00,  3.79it/s][A
All_batch: 4it [00:01,  3.89it/s][A
All_batch: 5it [00:01,  3.96it/s][A
All_batch: 6it [00:01,  3.72it/s][A
All_batch: 7it [00:01,  3.84it/s][A
All_batch: 8it [00:02,  4.00it/s][A
All_batch: 9it [00:02,  3.79it/s][A
All_batch: 10it [00:02,  3.82it/s][A
All_batch: 11it [00:02,  3.98it/s][A
All_batch: 12it [00:03,  3.86it/s][A
All_batch: 13it [00:03,  3.86it/s][A
All_batch: 14it [00:03,  4.00it/s][A
All_batch: 15it [00:03,  3.94it/s][A
All_batch: 16it [00:04,  3.90it/s][A
All_batch: 17it [00:04,  4.03it/s][A
All_batch: 18it [00:04,  4.00it/s][A
All_batch: 19it [00:04,  3.90it/s][A
All_batch: 20it [00:05,  4.02it/s][A
All_batch: 21it [00:05,  4.03it/s][A
All_batch: 22it [00:05,  3.99it/s][A
All_batch: 23it [00:05,  4.19it/s][A
All_batch: 24it [00:06,  4.04it/s][A
All_batch: 25it [00:06,  3.99it/s][A
All_batch: 26it [00:06,  4.19it/s][A
All_batch: 27it [00:06,  4.04it/s][A
All_batch: 28it [00:07,  3.93it/s][A
All_batch: 29it [00:07,  4.06it/s][A
All_batch: 30it [00:07,  3.93it/s][A
All_batch: 31it [00:07,  3.88it/s][AAll_batch: 31it [00:07,  3.95it/s]
[1;34mEpoch:43, Train_AUC:0.9659, Loss_rank:0.0870, Loss_reg:0.0005, Loss_ib:0.0008, Loss_sum:0.0882
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:43, topk:20, recall:0.1175, ndcg:0.0775, prec:0.0258, mrr:0.1142
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8472, valtime:1.2358

[1;35mTrain:[0m:   1%|[33m          [0m| 44/5000 [06:02<11:33:05,  8.39s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.28it/s][A
All_batch: 2it [00:00,  4.41it/s][A
All_batch: 3it [00:00,  3.94it/s][A
All_batch: 4it [00:00,  4.09it/s][A
All_batch: 5it [00:01,  4.25it/s][A
All_batch: 6it [00:01,  3.97it/s][A
All_batch: 7it [00:01,  4.13it/s][A
All_batch: 8it [00:01,  4.12it/s][A
All_batch: 9it [00:02,  3.90it/s][A
All_batch: 10it [00:02,  3.99it/s][A
All_batch: 11it [00:02,  4.13it/s][A
All_batch: 12it [00:02,  3.89it/s][A
All_batch: 13it [00:03,  4.00it/s][A
All_batch: 14it [00:03,  4.13it/s][A
All_batch: 15it [00:03,  3.86it/s][A
All_batch: 16it [00:04,  3.83it/s][A
All_batch: 17it [00:04,  3.94it/s][A
All_batch: 18it [00:04,  3.79it/s][A
All_batch: 19it [00:04,  3.79it/s][A
All_batch: 20it [00:05,  3.93it/s][A
All_batch: 21it [00:05,  3.91it/s][A
All_batch: 22it [00:05,  3.87it/s][A
All_batch: 23it [00:05,  3.99it/s][A
All_batch: 24it [00:06,  3.96it/s][A
All_batch: 25it [00:06,  3.86it/s][A
All_batch: 26it [00:06,  4.00it/s][A
All_batch: 27it [00:06,  4.02it/s][A
All_batch: 28it [00:07,  3.86it/s][A
All_batch: 29it [00:07,  4.07it/s][A
All_batch: 30it [00:07,  4.07it/s][A
All_batch: 31it [00:07,  3.89it/s][AAll_batch: 31it [00:07,  3.97it/s]
[1;34mEpoch:44, Train_AUC:0.9660, Loss_rank:0.0871, Loss_reg:0.0005, Loss_ib:0.0008, Loss_sum:0.0884
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:44, topk:20, recall:0.1178, ndcg:0.0775, prec:0.0260, mrr:0.1137
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8107, valtime:1.2837

[1;35mTrain:[0m:   1%|[33m          [0m| 45/5000 [06:12<11:50:23,  8.60s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 2it [00:00,  9.29it/s][A
All_batch: 3it [00:00,  8.75it/s][A
All_batch: 4it [00:00,  8.43it/s][A
All_batch: 5it [00:00,  8.30it/s][A
All_batch: 6it [00:00,  8.07it/s][A
All_batch: 7it [00:00,  7.85it/s][A
All_batch: 8it [00:00,  7.83it/s][A
All_batch: 9it [00:01,  7.78it/s][A
All_batch: 10it [00:01,  7.76it/s][A
All_batch: 11it [00:01,  7.75it/s][A
All_batch: 12it [00:01,  7.75it/s][A
All_batch: 13it [00:01,  7.79it/s][A
All_batch: 14it [00:01,  7.89it/s][A
All_batch: 15it [00:01,  7.95it/s][A
All_batch: 16it [00:01,  7.98it/s][A
All_batch: 17it [00:02,  8.02it/s][A
All_batch: 18it [00:02,  7.88it/s][A
All_batch: 19it [00:02,  7.93it/s][A
All_batch: 20it [00:02,  7.97it/s][A
All_batch: 21it [00:02,  8.02it/s][A
All_batch: 22it [00:02,  8.06it/s][A
All_batch: 23it [00:02,  8.05it/s][A
All_batch: 24it [00:02,  8.06it/s][A
All_batch: 25it [00:03,  8.08it/s][A
All_batch: 26it [00:03,  8.10it/s][A
All_batch: 27it [00:03,  8.10it/s][A
All_batch: 28it [00:03,  8.09it/s][A
All_batch: 29it [00:03,  8.09it/s][A
All_batch: 30it [00:03,  8.10it/s][A
All_batch: 31it [00:03,  8.13it/s][AAll_batch: 31it [00:03,  8.03it/s]
[1;34mEpoch:45, Train_AUC:0.9670, Loss_rank:0.0854, Loss_reg:0.0005, Loss_ib:0.0009, Loss_sum:0.0867
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:45, topk:20, recall:0.1169, ndcg:0.0774, prec:0.0258, mrr:0.1141
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:3.8588, valtime:1.2137

[1;35mTrain:[0m:   1%|[33m          [0m| 46/5000 [06:17<10:22:49,  7.54s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.11it/s][A
All_batch: 2it [00:00,  4.32it/s][A
All_batch: 3it [00:00,  3.89it/s][A
All_batch: 4it [00:00,  3.98it/s][A
All_batch: 5it [00:01,  4.23it/s][A
All_batch: 6it [00:01,  3.95it/s][A
All_batch: 7it [00:01,  4.00it/s][A
All_batch: 8it [00:01,  4.17it/s][A
All_batch: 9it [00:02,  3.88it/s][A
All_batch: 10it [00:02,  3.96it/s][A
All_batch: 11it [00:02,  4.02it/s][A
All_batch: 12it [00:03,  3.82it/s][A
All_batch: 13it [00:03,  3.87it/s][A
All_batch: 14it [00:03,  4.03it/s][A
All_batch: 15it [00:03,  3.87it/s][A
All_batch: 16it [00:04,  3.90it/s][A
All_batch: 17it [00:04,  4.00it/s][A
All_batch: 18it [00:04,  3.92it/s][A
All_batch: 19it [00:04,  3.93it/s][A
All_batch: 20it [00:05,  4.11it/s][A
All_batch: 21it [00:05,  3.95it/s][A
All_batch: 22it [00:05,  3.91it/s][A
All_batch: 23it [00:05,  4.08it/s][A
All_batch: 24it [00:06,  3.97it/s][A
All_batch: 25it [00:06,  3.95it/s][A
All_batch: 26it [00:06,  4.05it/s][A
All_batch: 27it [00:06,  3.99it/s][A
All_batch: 28it [00:07,  3.91it/s][A
All_batch: 29it [00:07,  4.10it/s][A
All_batch: 30it [00:07,  4.05it/s][A
All_batch: 31it [00:07,  3.91it/s][AAll_batch: 31it [00:07,  3.98it/s]
[1;34mEpoch:46, Train_AUC:0.9669, Loss_rank:0.0853, Loss_reg:0.0005, Loss_ib:0.0009, Loss_sum:0.0866
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:46, topk:20, recall:0.1173, ndcg:0.0774, prec:0.0258, mrr:0.1141
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.7926, valtime:1.2450

[1;35mTrain:[0m:   1%|[33m          [0m| 47/5000 [06:26<10:59:42,  7.99s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.29it/s][A
All_batch: 2it [00:00,  4.11it/s][A
All_batch: 3it [00:00,  3.82it/s][A
All_batch: 4it [00:00,  4.10it/s][A
All_batch: 5it [00:01,  4.08it/s][A
All_batch: 6it [00:01,  3.88it/s][A
All_batch: 7it [00:01,  4.08it/s][A
All_batch: 8it [00:01,  4.06it/s][A
All_batch: 9it [00:02,  3.87it/s][A
All_batch: 10it [00:02,  4.08it/s][A
All_batch: 11it [00:02,  4.07it/s][A
All_batch: 12it [00:03,  3.89it/s][A
All_batch: 13it [00:03,  4.08it/s][A
All_batch: 14it [00:03,  4.05it/s][A
All_batch: 15it [00:03,  3.88it/s][A
All_batch: 16it [00:03,  4.07it/s][A
All_batch: 17it [00:04,  4.05it/s][A
All_batch: 18it [00:04,  3.86it/s][A
All_batch: 19it [00:04,  3.93it/s][A
All_batch: 20it [00:04,  4.03it/s][A
All_batch: 21it [00:05,  3.77it/s][A
All_batch: 22it [00:05,  3.85it/s][A
All_batch: 23it [00:05,  3.99it/s][A
All_batch: 24it [00:06,  3.80it/s][A
All_batch: 25it [00:06,  3.91it/s][A
All_batch: 26it [00:06,  4.02it/s][A
All_batch: 27it [00:06,  3.86it/s][A
All_batch: 28it [00:07,  3.88it/s][A
All_batch: 29it [00:07,  4.03it/s][A
All_batch: 30it [00:07,  3.96it/s][A
All_batch: 31it [00:07,  3.95it/s][AAll_batch: 31it [00:07,  3.96it/s]
[1;34mEpoch:47, Train_AUC:0.9659, Loss_rank:0.0879, Loss_reg:0.0005, Loss_ib:0.0009, Loss_sum:0.0893
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:47, topk:20, recall:0.1170, ndcg:0.0777, prec:0.0258, mrr:0.1151
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8232, valtime:1.1983

[1;35mTrain:[0m:   1%|[33m          [0m| 48/5000 [06:35<11:25:04,  8.30s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.61it/s][A
All_batch: 2it [00:00,  4.18it/s][A
All_batch: 3it [00:00,  4.03it/s][A
All_batch: 4it [00:00,  4.24it/s][A
All_batch: 5it [00:01,  4.26it/s][A
All_batch: 6it [00:01,  4.58it/s][A
All_batch: 7it [00:01,  5.29it/s][A
All_batch: 8it [00:01,  4.94it/s][A
All_batch: 9it [00:01,  5.01it/s][A
All_batch: 10it [00:02,  5.47it/s][A
All_batch: 11it [00:02,  5.07it/s][A
All_batch: 12it [00:02,  5.09it/s][A
All_batch: 13it [00:02,  5.60it/s][A
All_batch: 14it [00:02,  5.22it/s][A
All_batch: 15it [00:03,  5.29it/s][A
All_batch: 16it [00:03,  5.54it/s][A
All_batch: 17it [00:03,  5.25it/s][A
All_batch: 18it [00:03,  5.34it/s][A
All_batch: 19it [00:03,  5.81it/s][A
All_batch: 20it [00:03,  5.35it/s][A
All_batch: 21it [00:04,  5.39it/s][A
All_batch: 22it [00:04,  5.85it/s][A
All_batch: 23it [00:04,  5.36it/s][A
All_batch: 24it [00:04,  5.36it/s][A
All_batch: 25it [00:04,  5.90it/s][A
All_batch: 26it [00:05,  5.46it/s][A
All_batch: 27it [00:05,  5.77it/s][A
All_batch: 28it [00:05,  5.71it/s][A
All_batch: 29it [00:05,  5.32it/s][A
All_batch: 30it [00:05,  5.67it/s][A
All_batch: 31it [00:05,  5.62it/s][AAll_batch: 31it [00:05,  5.26it/s]
[1;34mEpoch:48, Train_AUC:0.9667, Loss_rank:0.0854, Loss_reg:0.0005, Loss_ib:0.0010, Loss_sum:0.0868
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:48, topk:20, recall:0.1169, ndcg:0.0780, prec:0.0258, mrr:0.1158
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:5.8968, valtime:1.1855

[1;35mTrain:[0m:   1%|[33m          [0m| 49/5000 [06:42<10:54:47,  7.94s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  7.57it/s][A
All_batch: 2it [00:00,  5.20it/s][A
All_batch: 3it [00:00,  5.43it/s][A
All_batch: 4it [00:00,  5.13it/s][A
All_batch: 5it [00:01,  4.31it/s][A
All_batch: 6it [00:01,  4.23it/s][A
All_batch: 7it [00:01,  4.30it/s][A
All_batch: 8it [00:01,  4.03it/s][A
All_batch: 9it [00:02,  3.96it/s][A
All_batch: 10it [00:02,  4.10it/s][A
All_batch: 11it [00:02,  3.94it/s][A
All_batch: 12it [00:02,  3.91it/s][A
All_batch: 13it [00:03,  4.12it/s][A
All_batch: 14it [00:03,  4.00it/s][A
All_batch: 15it [00:03,  4.06it/s][A
All_batch: 16it [00:03,  4.24it/s][A
All_batch: 17it [00:04,  4.03it/s][A
All_batch: 18it [00:04,  4.05it/s][A
All_batch: 19it [00:04,  4.20it/s][A
All_batch: 20it [00:04,  3.98it/s][A
All_batch: 21it [00:05,  3.90it/s][A
All_batch: 22it [00:05,  4.07it/s][A
All_batch: 23it [00:05,  3.97it/s][A
All_batch: 24it [00:05,  3.92it/s][A
All_batch: 25it [00:06,  4.02it/s][A
All_batch: 26it [00:06,  3.88it/s][A
All_batch: 27it [00:06,  3.85it/s][A
All_batch: 28it [00:06,  3.97it/s][A
All_batch: 29it [00:07,  3.85it/s][A
All_batch: 30it [00:07,  3.87it/s][A
All_batch: 31it [00:07,  3.96it/s][AAll_batch: 31it [00:07,  4.09it/s]
[1;34mEpoch:49, Train_AUC:0.9676, Loss_rank:0.0837, Loss_reg:0.0005, Loss_ib:0.0010, Loss_sum:0.0852
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:49, topk:20, recall:0.1158, ndcg:0.0773, prec:0.0256, mrr:0.1144
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.5754, valtime:1.2049

[1;35mTrain:[0m:   1%|[33m          [0m| 50/5000 [06:51<11:15:34,  8.19s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  4.05it/s][A
All_batch: 2it [00:00,  3.68it/s][A
All_batch: 3it [00:00,  3.92it/s][A
All_batch: 4it [00:01,  4.09it/s][A
All_batch: 5it [00:01,  3.83it/s][A
All_batch: 6it [00:01,  3.88it/s][A
All_batch: 7it [00:01,  4.05it/s][A
All_batch: 8it [00:02,  3.89it/s][A
All_batch: 9it [00:02,  3.87it/s][A
All_batch: 10it [00:02,  4.06it/s][A
All_batch: 11it [00:02,  3.95it/s][A
All_batch: 12it [00:03,  3.89it/s][A
All_batch: 13it [00:03,  4.02it/s][A
All_batch: 14it [00:03,  3.92it/s][A
All_batch: 15it [00:03,  3.79it/s][A
All_batch: 16it [00:04,  3.87it/s][A
All_batch: 17it [00:04,  3.87it/s][A
All_batch: 18it [00:04,  3.75it/s][A
All_batch: 19it [00:04,  3.98it/s][A
All_batch: 20it [00:05,  3.98it/s][A
All_batch: 21it [00:05,  3.81it/s][A
All_batch: 22it [00:05,  4.02it/s][A
All_batch: 23it [00:05,  4.01it/s][A
All_batch: 24it [00:06,  3.84it/s][A
All_batch: 25it [00:06,  3.95it/s][A
All_batch: 26it [00:06,  4.08it/s][A
All_batch: 27it [00:06,  3.85it/s][A
All_batch: 28it [00:07,  3.93it/s][A
All_batch: 29it [00:07,  4.06it/s][A
All_batch: 30it [00:07,  3.91it/s][A
All_batch: 31it [00:07,  3.92it/s][AAll_batch: 31it [00:07,  3.92it/s]
[1;34mEpoch:50, Train_AUC:0.9667, Loss_rank:0.0844, Loss_reg:0.0005, Loss_ib:0.0010, Loss_sum:0.0859
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:50, topk:20, recall:0.1172, ndcg:0.0773, prec:0.0257, mrr:0.1137
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.9005, valtime:1.2028

[1;35mTrain:[0m:   1%|[33m          [0m| 51/5000 [07:00<11:38:04,  8.46s/it]
All_batch: 0it [00:00, ?it/s][A
All_batch: 1it [00:00,  3.96it/s][A
All_batch: 2it [00:00,  3.84it/s][A
All_batch: 3it [00:00,  3.62it/s][A
All_batch: 4it [00:01,  3.91it/s][A
All_batch: 5it [00:01,  3.96it/s][A
All_batch: 6it [00:01,  3.79it/s][A
All_batch: 7it [00:01,  4.00it/s][A
All_batch: 8it [00:02,  4.01it/s][A
All_batch: 9it [00:02,  3.84it/s][A
All_batch: 10it [00:02,  4.05it/s][A
All_batch: 11it [00:02,  4.05it/s][A
All_batch: 12it [00:03,  3.87it/s][A
All_batch: 13it [00:03,  4.04it/s][A
All_batch: 14it [00:03,  4.05it/s][A
All_batch: 15it [00:03,  3.84it/s][A
All_batch: 16it [00:04,  3.95it/s][A
All_batch: 17it [00:04,  4.08it/s][A
All_batch: 18it [00:04,  3.88it/s][A
All_batch: 19it [00:04,  4.01it/s][A
All_batch: 20it [00:05,  4.16it/s][A
All_batch: 21it [00:05,  3.93it/s][A
All_batch: 22it [00:05,  3.99it/s][A
All_batch: 23it [00:05,  4.10it/s][A
All_batch: 24it [00:06,  3.93it/s][A
All_batch: 25it [00:06,  3.91it/s][A
All_batch: 26it [00:06,  4.09it/s][A
All_batch: 27it [00:06,  3.84it/s][A
All_batch: 28it [00:07,  3.89it/s][A
All_batch: 29it [00:07,  4.11it/s][A
All_batch: 30it [00:07,  3.94it/s][A
All_batch: 31it [00:07,  3.92it/s][AAll_batch: 31it [00:07,  3.95it/s]
[1;34mEpoch:51, Train_AUC:0.9676, Loss_rank:0.0828, Loss_reg:0.0005, Loss_ib:0.0011, Loss_sum:0.0843
[0m[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;32mCurrent Eval: Epoch:51, topk:20, recall:0.1174, ndcg:0.0776, prec:0.0259, mrr:0.1141
[0m[1;31mBest Eval: Epoch:2, topk:20, recall:0.2407, ndcg:0.1712
[0mtraintime:7.8405, valtime:1.2327

early stop: 51
[1;32mmax_recall@20=:0.2407, max_ndcg@20=:0.1712
[0m[1;35mTrain:[0m:   1%|[33m          [0m| 51/5000 [07:09<11:34:09,  8.42s/it]
[DEBUG] User 1587: GT Size=4, Masking Size=32
[DEBUG] User 1252: GT Size=7, Masking Size=35
[DEBUG] User 684: GT Size=4, Masking Size=26
[DEBUG] User 160: GT Size=7, Masking Size=37
[DEBUG] User 1704: GT Size=4, Masking Size=29
[1;36mTopk:  5, HR:0.3640, Recall:0.1125, NDCG:0.1236, Prec:0.0961, MRR:0.2247
[0m[1;36mTopk: 10, HR:0.4781, Recall:0.1642, NDCG:0.1419, Prec:0.0706, MRR:0.2399
[0m[1;36mTopk: 20, HR:0.6135, Recall:0.2407, NDCG:0.1712, Prec:0.0509, MRR:0.2492
[0m[1;36mTopk: 30, HR:0.6771, Recall:0.2884, NDCG:0.1875, Prec:0.0412, MRR:0.2518
[0m[1;36mTopk: 40, HR:0.7231, Recall:0.3294, NDCG:0.1999, Prec:0.0353, MRR:0.2531
[0m[1;36mTopk: 50, HR:0.7533, Recall:0.3620, NDCG:0.2091, Prec:0.0310, MRR:0.2538
[0m[1;36mTopk: 60, HR:0.7758, Recall:0.3878, NDCG:0.2160, Prec:0.0276, MRR:0.2542
[0m[1;36mTopk: 70, HR:0.7966, Recall:0.4116, NDCG:0.2222, Prec:0.0252, MRR:0.2545
[0m[1;36mTopk: 80, HR:0.8147, Recall:0.4320, NDCG:0.2273, Prec:0.0231, MRR:0.2547
[0m[1;36mTopk: 90, HR:0.8295, Recall:0.4489, NDCG:0.2313, Prec:0.0213, MRR:0.2549
[0m[1;36mTopk:100, HR:0.8377, Recall:0.4636, NDCG:0.2350, Prec:0.0199, MRR:0.2550
[0mEND
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.009 MB uploadedwandb: | 0.006 MB of 0.040 MB uploadedwandb: / 0.040 MB of 0.040 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:        best/HR@10 ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       best/MRR@10 ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      best/NDCG@10 ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: best/Precision@10 ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    best/Recall@10 ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        best/epoch ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:         val/HR@10 ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:        val/MRR@10 ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:       val/NDCG@10 ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:  val/Precision@10 ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:     val/Recall@10 ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:         val/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:          val/loss ‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:        best/HR@10 0.47807
wandb:       best/MRR@10 0.23989
wandb:      best/NDCG@10 0.14189
wandb: best/Precision@10 0.07056
wandb:    best/Recall@10 0.16424
wandb:        best/epoch 2
wandb:         val/HR@10 0.27138
wandb:        val/MRR@10 0.10759
wandb:       val/NDCG@10 0.06273
wandb:  val/Precision@10 0.03514
wandb:     val/Recall@10 0.07969
wandb:         val/epoch 51
wandb:          val/loss 0.08433
wandb: 
wandb: üöÄ View run lastfm_0.001_2.0 at: https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/g6brqta2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251220_140950-g6brqta2/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2025-12-20 14:17:12,514 - wandb.wandb_agent - INFO - Cleaning up finished run: g6brqta2
2025-12-20 14:17:13,023 - wandb.wandb_agent - INFO - Agent received command: run
2025-12-20 14:17:13,023 - wandb.wandb_agent - INFO - Agent starting run with config:
	beta: 2
	dataset: lastfm
	device_id: 0
	lr: 0.0005
2025-12-20 14:17:13,024 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python run_GBSR.py --beta=2 --dataset=lastfm --device_id=0 --lr=0.0005
wandb: Currently logged in as: chtw2001 (chtw2001-hanyang-university). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/taekwon/SOTAs/GBSR/torch_version/wandb/run-20251220_141714-2hy8odkz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lastfm_0.0005_2.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: üßπ View sweep at https://wandb.ai/chtw2001-hanyang-university/GBSR/sweeps/t1zvf15e
wandb: üöÄ View run at https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/2hy8odkz
saving log to  ../saved/lastfm/GBSR/0/
dataset=lastfm
runid=0
device_id=0
epochs=5000
batch_size=2048
lr=0.0005
topk=20
early_stops=30
num_neg=1
gcn_layer=3
num_user=13024
num_item=22347
latent_dim=64
init_type=norm
l2_reg=0.0001
beta=2.0
sigma=0.25
edge_bias=0.5
social_noise_ratio=0
data_path=../datasets/lastfm/
Loading data from: ../datasets/lastfm/
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Successfully loaded social networks
Final Dataset Info: Users=13024, Items=22347, Total Nodes=35371
Total Training Interactions: 61978
Building negative sampler...
Negative sampler built.
Updated args.num_user to 13024
Updated args.num_item to 22347
user_embeddings.weight torch.Size([13024, 64])
item_embeddings.weight torch.Size([22347, 64])
linear_1.weight torch.Size([64, 128])
linear_1.bias torch.Size([64])
linear_2.weight torch.Size([1, 64])
linear_2.bias torch.Size([1])
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
All_batch: 0it [00:00, ?it/s][A2025-12-20 14:17:18,031 - wandb.wandb_agent - INFO - Running runs: ['2hy8odkz']
All_batch: 0it [00:01, ?it/s]
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "run_GBSR.py", line 123, in <module>
    total_loss.backward()
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.66 GiB (GPU 0; 47.53 GiB total capacity; 165.88 MiB already allocated; 3.75 GiB free; 210.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.009 MB uploadedwandb: | 0.011 MB of 0.011 MB uploadedwandb:                                                                                
wandb: üöÄ View run lastfm_0.0005_2.0 at: https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/2hy8odkz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251220_141714-2hy8odkz/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2025-12-20 14:17:28,479 - wandb.wandb_agent - INFO - Cleaning up finished run: 2hy8odkz
2025-12-20 14:17:28,843 - wandb.wandb_agent - INFO - Agent received command: run
2025-12-20 14:17:28,843 - wandb.wandb_agent - INFO - Agent starting run with config:
	beta: 2
	dataset: lastfm
	device_id: 0
	lr: 0.0001
2025-12-20 14:17:28,844 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python run_GBSR.py --beta=2 --dataset=lastfm --device_id=0 --lr=0.0001
wandb: Currently logged in as: chtw2001 (chtw2001-hanyang-university). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/taekwon/SOTAs/GBSR/torch_version/wandb/run-20251220_141730-5iby005n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lastfm_0.0001_2.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: üßπ View sweep at https://wandb.ai/chtw2001-hanyang-university/GBSR/sweeps/t1zvf15e
wandb: üöÄ View run at https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/5iby005n
saving log to  ../saved/lastfm/GBSR/0/
dataset=lastfm
runid=0
device_id=0
epochs=5000
batch_size=2048
lr=0.0001
topk=20
early_stops=30
num_neg=1
gcn_layer=3
num_user=13024
num_item=22347
latent_dim=64
init_type=norm
l2_reg=0.0001
beta=2.0
sigma=0.25
edge_bias=0.5
social_noise_ratio=0
data_path=../datasets/lastfm/
Loading data from: ../datasets/lastfm/
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Successfully loaded social networks
Final Dataset Info: Users=13024, Items=22347, Total Nodes=35371
Total Training Interactions: 61978
Building negative sampler...
Negative sampler built.
Updated args.num_user to 13024
Updated args.num_item to 22347
2025-12-20 14:17:33,855 - wandb.wandb_agent - INFO - Running runs: ['5iby005n']
user_embeddings.weight torch.Size([13024, 64])
item_embeddings.weight torch.Size([22347, 64])
linear_1.weight torch.Size([64, 128])
linear_1.bias torch.Size([64])
linear_2.weight torch.Size([1, 64])
linear_2.bias torch.Size([1])
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
All_batch: 0it [00:00, ?it/s][AAll_batch: 0it [00:01, ?it/s]
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "run_GBSR.py", line 123, in <module>
    total_loss.backward()
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.66 GiB (GPU 0; 47.53 GiB total capacity; 165.88 MiB already allocated; 3.75 GiB free; 210.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.007 MB of 0.011 MB uploadedwandb:                                                                                
wandb: üöÄ View run lastfm_0.0001_2.0 at: https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/5iby005n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251220_141730-5iby005n/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2025-12-20 14:17:44,328 - wandb.wandb_agent - INFO - Cleaning up finished run: 5iby005n
2025-12-20 14:17:44,693 - wandb.wandb_agent - INFO - Agent received command: run
2025-12-20 14:17:44,693 - wandb.wandb_agent - INFO - Agent starting run with config:
	beta: 2
	dataset: lastfm
	device_id: 0
	lr: 5e-05
2025-12-20 14:17:44,694 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python run_GBSR.py --beta=2 --dataset=lastfm --device_id=0 --lr=5e-05
wandb: Currently logged in as: chtw2001 (chtw2001-hanyang-university). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/taekwon/SOTAs/GBSR/torch_version/wandb/run-20251220_141746-4w6syhmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lastfm_5e-05_2.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: üßπ View sweep at https://wandb.ai/chtw2001-hanyang-university/GBSR/sweeps/t1zvf15e
wandb: üöÄ View run at https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/4w6syhmu
saving log to  ../saved/lastfm/GBSR/0/
dataset=lastfm
runid=0
device_id=0
epochs=5000
batch_size=2048
lr=5e-05
topk=20
early_stops=30
num_neg=1
gcn_layer=3
num_user=13024
num_item=22347
latent_dim=64
init_type=norm
l2_reg=0.0001
beta=2.0
sigma=0.25
edge_bias=0.5
social_noise_ratio=0
data_path=../datasets/lastfm/
Loading data from: ../datasets/lastfm/
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Successfully loaded social networks
Final Dataset Info: Users=13024, Items=22347, Total Nodes=35371
Total Training Interactions: 61978
Building negative sampler...
Negative sampler built.
Updated args.num_user to 13024
Updated args.num_item to 22347
2025-12-20 14:17:49,705 - wandb.wandb_agent - INFO - Running runs: ['4w6syhmu']
user_embeddings.weight torch.Size([13024, 64])
item_embeddings.weight torch.Size([22347, 64])
linear_1.weight torch.Size([64, 128])
linear_1.bias torch.Size([64])
linear_2.weight torch.Size([1, 64])
linear_2.bias torch.Size([1])
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
All_batch: 0it [00:00, ?it/s][AAll_batch: 0it [00:01, ?it/s]
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "run_GBSR.py", line 123, in <module>
    total_loss.backward()
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.66 GiB (GPU 0; 47.53 GiB total capacity; 165.88 MiB already allocated; 3.75 GiB free; 210.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.011 MB uploadedwandb: | 0.011 MB of 0.011 MB uploadedwandb:                                                                                
wandb: üöÄ View run lastfm_5e-05_2.0 at: https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/4w6syhmu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251220_141746-4w6syhmu/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2025-12-20 14:18:00,190 - wandb.wandb_agent - INFO - Cleaning up finished run: 4w6syhmu
2025-12-20 14:18:00,585 - wandb.wandb_agent - INFO - Agent received command: run
2025-12-20 14:18:00,585 - wandb.wandb_agent - INFO - Agent starting run with config:
	beta: 2
	dataset: ciao
	device_id: 0
	lr: 0.001
2025-12-20 14:18:00,586 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python run_GBSR.py --beta=2 --dataset=ciao --device_id=0 --lr=0.001
wandb: Currently logged in as: chtw2001 (chtw2001-hanyang-university). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/taekwon/SOTAs/GBSR/torch_version/wandb/run-20251220_141802-1skcy6wz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ciao_0.001_2.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: üßπ View sweep at https://wandb.ai/chtw2001-hanyang-university/GBSR/sweeps/t1zvf15e
wandb: üöÄ View run at https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/1skcy6wz
saving log to  ../saved/ciao/GBSR/0/
dataset=ciao
runid=0
device_id=0
epochs=5000
batch_size=2048
lr=0.001
topk=20
early_stops=30
num_neg=1
gcn_layer=3
num_user=13024
num_item=22347
latent_dim=64
init_type=norm
l2_reg=0.0001
beta=2.0
sigma=0.25
edge_bias=0.5
social_noise_ratio=0
data_path=../datasets/ciao/
Loading data from: ../datasets/ciao/
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Successfully loaded social networks
Final Dataset Info: Users=13024, Items=22347, Total Nodes=35371
Total Training Interactions: 112502
Building negative sampler...
Negative sampler built.
Updated args.num_user to 13024
Updated args.num_item to 22347
user_embeddings.weight torch.Size([13024, 64])
item_embeddings.weight torch.Size([22347, 64])
linear_1.weight torch.Size([64, 128])
linear_1.bias torch.Size([64])
linear_2.weight torch.Size([1, 64])
linear_2.bias torch.Size([1])
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
All_batch: 0it [00:00, ?it/s][AAll_batch: 0it [00:00, ?it/s]
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "run_GBSR.py", line 123, in <module>
    total_loss.backward()
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.66 GiB (GPU 0; 47.53 GiB total capacity; 242.51 MiB already allocated; 3.68 GiB free; 284.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-12-20 14:18:05,597 - wandb.wandb_agent - INFO - Running runs: ['1skcy6wz']
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.007 MB of 0.011 MB uploadedwandb: | 0.011 MB of 0.011 MB uploadedwandb:                                                                                
wandb: üöÄ View run ciao_0.001_2.0 at: https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/1skcy6wz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251220_141802-1skcy6wz/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2025-12-20 14:18:16,049 - wandb.wandb_agent - INFO - Cleaning up finished run: 1skcy6wz
2025-12-20 14:18:16,398 - wandb.wandb_agent - INFO - Agent received command: run
2025-12-20 14:18:16,398 - wandb.wandb_agent - INFO - Agent starting run with config:
	beta: 2
	dataset: ciao
	device_id: 0
	lr: 0.0005
2025-12-20 14:18:16,399 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python run_GBSR.py --beta=2 --dataset=ciao --device_id=0 --lr=0.0005
wandb: Currently logged in as: chtw2001 (chtw2001-hanyang-university). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/taekwon/SOTAs/GBSR/torch_version/wandb/run-20251220_141818-b9u91kia
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ciao_0.0005_2.0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: üßπ View sweep at https://wandb.ai/chtw2001-hanyang-university/GBSR/sweeps/t1zvf15e
wandb: üöÄ View run at https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/b9u91kia
saving log to  ../saved/ciao/GBSR/0/
dataset=ciao
runid=0
device_id=0
epochs=5000
batch_size=2048
lr=0.0005
topk=20
early_stops=30
num_neg=1
gcn_layer=3
num_user=13024
num_item=22347
latent_dim=64
init_type=norm
l2_reg=0.0001
beta=2.0
sigma=0.25
edge_bias=0.5
social_noise_ratio=0
data_path=../datasets/ciao/
Loading data from: ../datasets/ciao/
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Detected format: N x 2 Interaction Matrix
Successfully loaded social networks
Final Dataset Info: Users=13024, Items=22347, Total Nodes=35371
Total Training Interactions: 112502
Building negative sampler...
Negative sampler built.
Updated args.num_user to 13024
Updated args.num_item to 22347
2025-12-20 14:18:21,409 - wandb.wandb_agent - INFO - Running runs: ['b9u91kia']
user_embeddings.weight torch.Size([13024, 64])
item_embeddings.weight torch.Size([22347, 64])
linear_1.weight torch.Size([64, 128])
linear_1.bias torch.Size([64])
linear_2.weight torch.Size([1, 64])
linear_2.bias torch.Size([1])
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:00<?, ?it/s]
All_batch: 0it [00:00, ?it/s][AAll_batch: 0it [00:01, ?it/s]
[1;35mTrain:[0m:   0%|[33m          [0m| 0/5000 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "run_GBSR.py", line 123, in <module>
    total_loss.backward()
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/taekwon/.conda/envs/GBSR/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.66 GiB (GPU 0; 47.53 GiB total capacity; 242.51 MiB already allocated; 3.68 GiB free; 284.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.007 MB of 0.011 MB uploadedwandb:                                                                                
wandb: üöÄ View run ciao_0.0005_2.0 at: https://wandb.ai/chtw2001-hanyang-university/GBSR/runs/b9u91kia
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/chtw2001-hanyang-university/GBSR
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251220_141818-b9u91kia/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2025-12-20 14:18:31,850 - wandb.wandb_agent - ERROR - Detected 5 failed runs in a row, shutting down.
2025-12-20 14:18:31,850 - wandb.wandb_agent - INFO - To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val
wandb: Terminating and syncing runs. Press ctrl-c to kill.
